{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World, TurboZero Backgammon 🏁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`turbozero` provides a vectorized implementation of AlphaZero. \n",
    "\n",
    "In a nutshell, this means we can massively speed up training, by collecting many self-play games and running Monte Carlo Tree Search in parallel across one or more GPUs!\n",
    "\n",
    "As the user, you just need to provide:\n",
    "* environment dynamics functions (step and init) that adhere to the TurboZero spec\n",
    "* a conversion function for environment state -> neural net input\n",
    "* and a few hyperparameters!\n",
    "\n",
    "TurboZero takes care of the rest. 😀 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Follow the instructions in the repo readme to properly install dependencies and set up your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "In order to take advantage of the batched implementation of AlphaZero, we need to pair it with a vectorized environment.\n",
    "\n",
    "Fortunately, there are many great vectorized RL environment libraries, one I like in particular is [pgx](https://github.com/sotetsuk/pgx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 3.12.10 (main, May 21 2025, 10:26:13) [GCC 13.3.0]\n",
      "Jax Version: 0.6.0, Backend: gpu \n",
      "PGX Version 2.5.10\n",
      "Backgammon version: v2 Num Actions: 156 Simple Doubles: False\n",
      "[0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778\n",
      " 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
      " 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
      " 0.05555556 0.05555556 0.05555556]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg baseProfile=\"full\" height=\"375.0\" version=\"1.1\" width=\"450.0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><rect fill=\"white\" height=\"425\" width=\"575\" x=\"0\" y=\"0\" /><g transform=\"scale(1.0)\"><rect fill=\"white\" height=\"375\" width=\"450\" x=\"0\" y=\"0\" /><g transform=\"translate(12.5,12.5)\"><polygon fill=\"white\" points=\"0,0 25,0 12.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"25,0 50,0 37.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"50,0 75,0 62.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"75,0 100,0 87.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"100,0 125,0 112.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"125,0 150,0 137.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"175,0 200,0 187.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"200,0 225,0 212.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"225,0 250,0 237.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"250,0 275,0 262.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"275,0 300,0 287.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"300,0 325,0 312.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"0,350 25,350 12.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"25,350 50,350 37.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"50,350 75,350 62.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"75,350 100,350 87.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"100,350 125,350 112.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"125,350 150,350 137.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"175,350 200,350 187.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"200,350 225,350 212.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"225,350 250,350 237.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"250,350 275,350 262.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"275,350 300,350 287.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"300,350 325,350 312.5,200\" stroke=\"gray\" /><rect fill=\"none\" height=\"350\" stroke=\"black\" width=\"325\" x=\"0\" y=\"0\" /><rect fill=\"none\" height=\"350\" stroke=\"black\" width=\"25\" x=\"150\" y=\"0\" /><circle cx=\"287.5\" cy=\"337.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"262.5\" cy=\"337.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"262.5\" cy=\"312.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"262.5\" cy=\"287.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"212.5\" cy=\"337.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"212.5\" cy=\"312.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"337.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"312.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"287.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"337.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"312.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"287.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"87.5\" cy=\"337.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"87.5\" cy=\"312.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"337.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"12.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"87.5\" cy=\"12.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"87.5\" cy=\"37.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"12.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"37.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"62.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"12.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"37.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"62.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"212.5\" cy=\"12.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"212.5\" cy=\"37.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"262.5\" cy=\"12.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"262.5\" cy=\"37.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"262.5\" cy=\"62.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"287.5\" cy=\"12.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><rect fill=\"white\" height=\"50\" stroke=\"black\" width=\"100\" x=\"325\" y=\"0\" /><circle cx=\"350\" cy=\"25\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><text fill=\"black\" font-family=\"serif\" font-size=\"34px\" x=\"365.0\" y=\"35.0\">×0</text><rect fill=\"white\" height=\"50\" stroke=\"black\" width=\"100\" x=\"325\" y=\"300\" /><circle cx=\"350\" cy=\"325\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><text fill=\"black\" font-family=\"serif\" font-size=\"34px\" x=\"365.0\" y=\"335.0\">×0</text><text fill=\"black\" font-family=\"sans serif\" font-size=\"44px\" x=\"337.5\" y=\"187.5\">⚀</text><text fill=\"black\" font-family=\"sans serif\" font-size=\"44px\" x=\"370.0\" y=\"187.5\">⚃</text></g></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Only if needed\n",
    "#!pip install git+https://github.com/sile16/turbozero.git\n",
    "\n",
    "import sys\n",
    "print(f\"Python version {sys.version}\")\n",
    "\n",
    "#jax.config.update('jax_platform_name', 'gpu')\n",
    "import jax\n",
    "\n",
    "from prompt_toolkit import HTML\n",
    "\n",
    "import pgx\n",
    "import pgx.backgammon as bg\n",
    "import os\n",
    "\n",
    "#print(f\"Env: {os.environ}\")\n",
    "print(f\"Jax Version: {jax.__version__}, Backend: {jax.default_backend()} \")\n",
    "print(f\"PGX Version {pgx.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = bg.Backgammon(short_game=True)\n",
    "\n",
    "print(f\"Backgammon version: {env.version} Num Actions: {env.num_actions} Simple Doubles: {env.simple_doubles}\" )\n",
    "print(env.stochastic_action_probs)\n",
    "\n",
    "# create key\n",
    "key = jax.random.PRNGKey(0)\n",
    "state = env.init(key)\n",
    "from IPython.display import HTML\n",
    "display(HTML(state.to_svg()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Dynamics\n",
    "\n",
    "Turbozero needs to interface with the environment in order to build search trees and collect self-play episodes.\n",
    "\n",
    "We can define this interface with the following functions:\n",
    "* `env_step_fn`: given an environment state and an action, return the new environment state \n",
    "```python\n",
    "    EnvStepFn = Callable[[chex.ArrayTree, int], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "* `env_init_fn`: given a key, initialize and reutrn a new environment state\n",
    "```python\n",
    "    EnvInitFn = Callable[[chex.PRNGKey], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "Fortunately, environment libraries implement these for us! We just need to extract a few key pieces of information \n",
    "from the environment state so that we can match the TurboZero specification. We store this in a StepMetadata object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.types import StepMetadata\n",
    "#%psource StepMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `rewards` stores the rewards emitted for each player for the given timestep\n",
    "* `action_mask` is a mask across all possible actions, where legal actions are set to `True`, and invalid/illegal actions are set to `False`\n",
    "* `terminated` True if the environment is terminated/completed\n",
    "* `cur_player_id`: id of the current player\n",
    "* `step`: step number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the environment interface for `Backgammon` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chex\n",
    "from core.bgcommon import bg_step_fn\n",
    "from functools import partial\n",
    "step_fn = partial(bg_step_fn, env)\n",
    "\n",
    "def init_fn(key):\n",
    "    \"\"\"Initializes a new environment state.\"\"\"\n",
    "    state = env.init(key)\n",
    "    # No need to force non-stochastic, let the environment handle it\n",
    "    return state, StepMetadata(\n",
    "        rewards=state.rewards,\n",
    "        action_mask=state.legal_action_mask,\n",
    "        terminated=state.terminated,\n",
    "        cur_player_id=state.current_player,\n",
    "        step=state._step_count\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Next, we'll need to define the architecture of the neural network \n",
    "\n",
    "A simple implementation of the residual neural network used in the _AlphaZero_ paper is included for your convenience. \n",
    "\n",
    "You can implement your own architecture using `flax.linen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "# Pre‑activation ResNet‑V2 block\n",
    "class ResBlockV2(nn.Module):\n",
    "    features: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        r = x\n",
    "        x = nn.LayerNorm()(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.features, use_bias=False)(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.features, use_bias=False)(x)\n",
    "        return x + r\n",
    "\n",
    "class ResNetTurboZero(nn.Module):\n",
    "    num_actions: int            # 156 here\n",
    "    hidden_dim: int = 256\n",
    "    num_blocks: int = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = False):\n",
    "        # 1) ResNet tower\n",
    "        x = nn.Dense(self.hidden_dim, use_bias=False)(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "        x = nn.relu(x)\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = ResBlockV2(self.hidden_dim)(x)\n",
    "\n",
    "        # 2) Policy head: single Dense into 156 logits\n",
    "        policy_logits = nn.Dense(self.num_actions)(x)\n",
    "\n",
    "        # 3) Value head\n",
    "        v = nn.LayerNorm()(x)\n",
    "        v = nn.relu(v)\n",
    "        v = nn.Dense(1)(v)\n",
    "        v = jnp.squeeze(v, -1)\n",
    "\n",
    "        return policy_logits, v\n",
    "    \n",
    "\n",
    "resnet_model = ResNetTurboZero(\n",
    "    num_actions=env.num_actions,  # i.e. micro_steps*(micro_src + micro_die)\n",
    "    hidden_dim=256,\n",
    "    num_blocks=10\n",
    ")\n",
    "\n",
    "from core.networks.mlp import MLPConfig, MLP\n",
    "\n",
    "# Replace the resnet with an MLP network\n",
    "mlp_network = MLP(MLPConfig(\n",
    "    hidden_dims=[128, 128, 64],  # Adjust layer sizes as needed\n",
    "    policy_head_out_size=env.num_actions,\n",
    "    value_head_out_size=1\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to convert our environment's state into something our neural network can take as input (i.e. structured data -> Array). `pgx` conveniently includes this in `state.observation`, but for other environments you may need to perform the conversion yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.networks.azresnet import AZResnetConfig, AZResnet\n",
    "\n",
    "resnet_simple = AZResnet(AZResnetConfig(\n",
    "    policy_head_out_size=env.num_actions,\n",
    "    num_blocks=4,\n",
    "    num_channels=32,\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_nn_input(state):\n",
    "    return state.observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Next, we can initialize our evaluator, AlphaZero, which takes the following parameters:\n",
    "\n",
    "* `eval_fn`: function used to evaluate a leaf node (returns a policy and value)\n",
    "* `num_iterations`: number of MCTS iterations to run before returning the final policy\n",
    "* `max_nodes`: maximum capacity of search tree\n",
    "* `branching_factor`: branching factor of search tree == policy_size\n",
    "* `action_selector`: the algorithm used to select an action to take at any given search node, choose between:\n",
    "    * `PUCTSelector`: AlphaZero action selection algorithm\n",
    "    * `MuZeroPUCTSelector`: MuZero action selection algorithm\n",
    "    * or write your own! :)\n",
    "\n",
    "There are also a few other optional parameters, a few of the important ones are:\n",
    "* `temperature`: temperature applied to move probabilities prior to sampling (0.0 == argmax, ->inf == completely random sampling). I reccommend setting this to 1.0 for training (default) and 0.0 for evaluation.\n",
    "* `dirichlet_alpha`: magnitude of Dirichlet noise to add to root policy (default 0.3). Generally, the more actions are possible in a game, the smaller this value should be. \n",
    "* `dirichlet_epsilon`: proportion of root policy composed of Dirichlet noise (default 0.25)\n",
    "\n",
    "\n",
    "We use `make_nn_eval_fn` to create a leaf evaluation function that uses our neural network to generate a policy and a value for the given state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from core.evaluators.evaluation_fns import make_nn_eval_fn\n",
    "from core.evaluators.mcts.action_selection import PUCTSelector\n",
    "from core.evaluators.mcts.stochastic_mcts import StochasticMCTS\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Training evaluator: StochasticMCTS using NN\n",
    "evaluator = StochasticMCTS(   #Explores new moves\n",
    "    eval_fn=make_nn_eval_fn(model, state_to_nn_input),\n",
    "    stochastic_action_probs=env.stochastic_action_probs,\n",
    "    num_iterations=1600,  \n",
    "    max_nodes=2000,      \n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector=PUCTSelector(),\n",
    "    temperature=0.99,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a separate evaluator with different parameters to use for testing purposes. We'll give this one a larger budget (num_iterations), and set the temperature to zero so it always chooses the most-visited action after search is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluator_test = StochasticMCTS(   #Use optimized moves, temperature=0.0\n",
    "    eval_fn=make_nn_eval_fn(model, state_to_nn_input),\n",
    "    stochastic_action_probs=env.stochastic_action_probs,\n",
    "    num_iterations=300,  # Very few iterations\n",
    "    max_nodes=1000,      # Very small tree\n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector=PUCTSelector(),\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use similar ideas to write a greedy baseline evaluation function, one that doesn't use a neural network at all!\n",
    "\n",
    "Instead, it simply counts the number of tiles for the active player and compares it to the number of tiles controlled by the other player, so the value is higher for states where the active player controls more tiles than the other player.\n",
    "\n",
    "Using similar techniques as before, we can create another AlphaZero evaluator to test against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.evaluators.evaluation_fns import make_nn_eval_fn_no_params_callable\n",
    "import chex\n",
    "from core.bgcommon import bg_hit2_eval\n",
    "\n",
    "\n",
    "# Test evaluator: Regular MCTS using pip count\n",
    "bg_hit2_mcts_evaluator_test = StochasticMCTS(  # optimizes for moves\n",
    "    eval_fn=bg_hit2_eval, # Use pip count eval fn\n",
    "    stochastic_action_probs=env.stochastic_action_probs,\n",
    "    num_iterations=30, # Give it slightly more iterations maybe\n",
    "    max_nodes=100,\n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector=PUCTSelector(),\n",
    "    temperature=0.0 # Deterministic action selection for testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering\n",
    "We can optionally provide a `render_fn` that will record games played by our model against one of the baselines and save it as a `.gif`.\n",
    "\n",
    "I've included a helper fn that takes care of this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function depends upon cairoSVG, which itself depends upon `cairo`, which you'll need to install on your system.\n",
    "\n",
    "On Ubuntu, this can be done with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! apt-get update && apt-get -y install libcairo2-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're on another OS, consult https://www.cairographics.org/download/ for installation guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from core.testing.utils import render_pgx_2p\n",
    "render_fn = partial(render_pgx_2p, p1_label='Black', p2_label='White', duration=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temperature(train_steps) -> float:\n",
    "    \n",
    "    if train_steps < 1e5:\n",
    "        return 1.0\n",
    "    elif train_steps < 2e5:\n",
    "        return 0.5\n",
    "    elif train_steps < 3e5:\n",
    "        return 0.1\n",
    "    else:\n",
    "        # Greedy selection.\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory Buffer\n",
    "\n",
    "Next, we'll initialize a replay memory buffer to hold selfplay trajectories that we can sample from during training. This actually just defines an interface, the buffer state itself will be initialized and managed internally.\n",
    "\n",
    "The replay buffer is batched, it retains a buffer of trajectories across a batch dimension. We specify a `capacity`: the amount of samples stored in a single buffer. The total capacity of the entire replay buffer is then `batch_size * capacity`, where `batch_size` is the number of environments/self-play games being run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replay buffer size: 32768\n"
     ]
    }
   ],
   "source": [
    "from core.memory.replay_memory import EpisodeReplayBuffer\n",
    "collect_batch_size = 32\n",
    "collection_steps_per_epoch = 1024\n",
    "\n",
    "buffer_size = collect_batch_size * collection_steps_per_epoch * 1 # buffer holds 4 epochs, \n",
    "\n",
    "replay_memory = EpisodeReplayBuffer(capacity=buffer_size )\n",
    "print(f\"replay buffer size: {buffer_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Initialization\n",
    "Now that we have all the proper pieces defined, we are ready to initialize a Trainer and start training!\n",
    "\n",
    "The `Trainer` takes many parameters, so let's walk through them all:\n",
    "* `batch_size`: # of parallel environments used to collect self-play games\n",
    "* `train_batch_size`: size of minibatch used during training step\n",
    "* `warmup_steps`: # of steps (per batch) to collect via self-play prior to entering the training loop. This is used to populate the replay memory with some initial samples\n",
    "* `collection_steps_per_epoch`: # of steps (per batch) to collect via self-play per epoch\n",
    "* `train_steps_per_epoch`: # of train steps per epoch\n",
    "* `nn`: neural network (`linen.Module`)\n",
    "* `loss_fn`: loss function used for training, we use a provided default loss which implements the loss function used in the `AlphaZero` paper\n",
    "* `optimizer`: an `optax` optimizer used for training\n",
    "* `evaluator`: the `Evaluator` to use during self-play, we initialized ours using `AlphaZero(MCTS)`\n",
    "* `memory_buffer`: the memory buffer used to store samples from self-play games, we  initialized ours using `EpisodeReplayBuffer`\n",
    "* `max_episode_steps`: maximum number of steps/turns to allow before truncating an episode\n",
    "* `env_step_fn`: environment step function (we defined ours above)\n",
    "* `env_init_fn`: environment init function (we defined ours above)\n",
    "* `state_to_nn_input_fn`: function to convert environment state to nn input (we defined ours above)\n",
    "* `testers`: any number of `Tester`s, used to evaluate a given model and take their own parameters. We'll use the two evaluators defined above to initialize two Testers.\n",
    "* `evaluator_test`: (Optional) Evaluator used within Testers. By default used `evaluator`, but sometimes you may want to test with a larger MCTS iteration budget for example, or a different move sampling temperature\n",
    "* `data_transform_fns`: (optional) list of data transform functions to apply to self-play experiences (e.g. rotation, reflection, etc.)\n",
    "* `extract_model_params_fn`: (Optional) in special cases we need to define how to extract all model parameters from a flax `TrainState`. The default function handles BatchNorm, but if another special-case technique applied across batches is used (e.g. Dropout) we would need to define a function to extract the appropriate parameters. You usually won't need to define this!\n",
    "* `wandb_project_name`: (Optional) Weights and Biases project name. You will be prompted to login if a name is provided. If a name is provided, a run will be initialized and loss and other metrics will be logged to the given wandb project.\n",
    "* `ckpt_dir`: (Optional) directory to store checkpoints in, by default this is set to `/tmp/turbozero_checkpoints`\n",
    "* `max_checkpoints`: (Optional) maximum number of most-recent checkpoints to retain (default: 2)\n",
    "* `num_devices`: (Optional) number of hardware accelerators (GPUs/TPUs) to use. If not given, all available hardware accelerators are used\n",
    "* `wandb_run`: (Optional) continues from an initialized `wandb` run if provided, otherwise a new one is initialized\n",
    "* `extra_wandb_config`: (Optional) any extra metadata to store in the `wandb` run config\n",
    "\n",
    "A training epoch is comprised of M collection steps, followed by N training steps sampling minibatches from replay memory. Optionally, any number of Testers evaluate the current model. At the end of each epoch, a checkpoint is saved.\n",
    "\n",
    "If you are using one or more GPUs (reccommended), TurboZero by default will run on all your available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect_batch: 32 Total_Collected_steps_per_epoch: 32768\n",
      "train_batch_size=1024 train_steps: 32\n",
      "Ratio of trainstep to collect steps: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msile16\u001b[0m (\u001b[33msile16-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile/github/turbozero/notebooks/wandb/run-20250527_115110-4lsa0anv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sile16-self/TurboZero-Backgammon-Notebook/runs/4lsa0anv' target=\"_blank\">confused-butterfly-36</a></strong> to <a href='https://wandb.ai/sile16-self/TurboZero-Backgammon-Notebook' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sile16-self/TurboZero-Backgammon-Notebook' target=\"_blank\">https://wandb.ai/sile16-self/TurboZero-Backgammon-Notebook</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sile16-self/TurboZero-Backgammon-Notebook/runs/4lsa0anv' target=\"_blank\">https://wandb.ai/sile16-self/TurboZero-Backgammon-Notebook/runs/4lsa0anv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from core.testing.two_player_baseline import TwoPlayerBaseline\n",
    "from core.training.loss_fns import az_default_loss_fn\n",
    "from core.training.stochastic_train import StochasticTrainer\n",
    "import optax\n",
    "\n",
    "train_ratio = 1\n",
    "total_collected_per_epoch = collect_batch_size * collection_steps_per_epoch # 2048 so to minimize games truncated to < 10%\n",
    "train_batch_size = 1024 \n",
    "train_steps = int(train_ratio * total_collected_per_epoch / train_batch_size) # train steps vs collect steps\n",
    "\n",
    "print(f\"Collect_batch: {collect_batch_size} Total_Collected_steps_per_epoch: {total_collected_per_epoch}\")\n",
    "print(f\"train_batch_size={train_batch_size} train_steps: {train_steps}\")\n",
    "print(f\"Ratio of trainstep to collect steps: {train_ratio}\")\n",
    "\n",
    "\n",
    "\n",
    "trainer = StochasticTrainer(\n",
    "    warmup_steps=0,\n",
    "    batch_size=collect_batch_size,\n",
    "    collection_steps_per_epoch=collection_steps_per_epoch,\n",
    "    train_batch_size=train_batch_size,\n",
    "    train_steps_per_epoch=train_steps,  #       \n",
    "    nn=model,\n",
    "    loss_fn=partial(az_default_loss_fn, l2_reg_lambda=1e-4),\n",
    "    optimizer=optax.adam(1e-4),\n",
    "    # Use the stochastic evaluator for training\n",
    "    evaluator=evaluator, \n",
    "    memory_buffer=replay_memory,\n",
    "    max_episode_steps=500,  # should be enough for most games\n",
    "    env_step_fn=step_fn,\n",
    "    env_init_fn=init_fn,\n",
    "    state_to_nn_input_fn=state_to_nn_input,\n",
    "    ckpt_dir = \"/tmp/ckpts\",\n",
    "    testers=[\n",
    "        # Use our custom BackgammonTwoPlayerBaseline\n",
    "        TwoPlayerBaseline(\n",
    "            num_episodes=30,\n",
    "            baseline_evaluator=bg_hit2_mcts_evaluator_test,\n",
    "            #render_fn=render_fn,\n",
    "            #render_dir='training_eval/pip_count_baseline',\n",
    "            name='hit2_eval'\n",
    "        )\n",
    "    ],\n",
    "    # Use the pip count MCTS evaluator for testing\n",
    "    evaluator_test=evaluator_test, \n",
    "    data_transform_fns=[],  # No data transforms as requested\n",
    "    wandb_project_name=\"TurboZero-Backgammon-Notebook\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now all that's left to do is to kick off the training loop! We need to pass an initial seed for reproducibility, and the number of epochs to run for!\n",
    "\n",
    "If you've set up `wandb`, you can track metrics via plots in the run dashboard. Metrics will also be printed to the console. \n",
    "\n",
    "IMPORTANT: The first epoch will not execute quickly! This is because there is significant overhead in JAX compilation (nearly all of the training loop is JIT-compiled). This will cause the first epoch to run very slowly, as JIT-compiled functions are traced and compiled the first time they are run. Expect epochs after the first to execute much more quickly. Typically, GPU utilization will also be low/zero during this period.\n",
    "\n",
    "It's also worth mentioning that the hyperparameters in this notebook are just here for example purposes. Regardless of the task, they will need to be tuned according to the characteristics of the environment as well as your available hardware and time/cost constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 1.0\n",
      "Collecting self-play games\n",
      "Epoch 0: {'hit2_eval_avg_outcome': '-0.1333', 'perf/test_hit2_eval_time_sec': '347.9156'}\n",
      "Temperature: 1.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 3: {'l2_reg': '1.1024', 'loss': '2.7417', 'policy_entropy': '0.9696', 'policy_loss': '1.1116', 'value_loss': '0.5277', 'train/train_time_sec': '6.1123', 'train/train_steps_per_sec': '5360.9985', 'collect/temperature': '1.0000', 'collect/collect_time_sec': '1059.5687', 'collect/collect_steps_per_sec': '0.9664', 'buffer/populated': '90760.0000', 'buffer/has_reward': '1046561.0000', 'buffer/fullness_pct': '8.6555', 'perf/epoch_time_sec': '1065.6863'}\n",
      "Temperature: 0.5\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 4: {'l2_reg': '1.0977', 'loss': '2.7256', 'policy_entropy': '0.9731', 'policy_loss': '1.0908', 'value_loss': '0.5371', 'train/train_time_sec': '6.0423', 'train/train_steps_per_sec': '5423.1011', 'collect/temperature': '0.5000', 'collect/collect_time_sec': '1070.8238', 'collect/collect_steps_per_sec': '0.9563', 'buffer/populated': '113497.0000', 'buffer/has_reward': '1046420.0000', 'buffer/fullness_pct': '10.8239', 'perf/epoch_time_sec': '1076.8714'}\n",
      "Temperature: 0.5\n",
      "Collecting self-play games\n",
      "Epoch 5: {'hit2_eval_avg_outcome': '0.1000', 'perf/test_hit2_eval_time_sec': '294.5092'}\n",
      "Temperature: 0.5\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 6: {'l2_reg': '1.0882', 'loss': '2.6914', 'policy_entropy': '0.9579', 'policy_loss': '1.0577', 'value_loss': '0.5455', 'train/train_time_sec': '6.0279', 'train/train_steps_per_sec': '5436.0889', 'collect/temperature': '0.5000', 'collect/collect_time_sec': '1054.5914', 'collect/collect_steps_per_sec': '0.9710', 'buffer/populated': '158870.0000', 'buffer/has_reward': '1045936.0000', 'buffer/fullness_pct': '15.1510', 'perf/epoch_time_sec': '1060.6251'}\n",
      "Temperature: 0.1\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 7: {'l2_reg': '1.0835', 'loss': '2.6866', 'policy_entropy': '0.9716', 'policy_loss': '1.0635', 'value_loss': '0.5396', 'train/train_time_sec': '6.0755', 'train/train_steps_per_sec': '5393.4238', 'collect/temperature': '0.1000', 'collect/collect_time_sec': '1057.7824', 'collect/collect_steps_per_sec': '0.9681', 'buffer/populated': '181546.0000', 'buffer/has_reward': '1046355.0000', 'buffer/fullness_pct': '17.3136', 'perf/epoch_time_sec': '1063.8635'}\n",
      "Temperature: 0.1\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 8: {'l2_reg': '1.0788', 'loss': '2.6639', 'policy_entropy': '0.9731', 'policy_loss': '1.0575', 'value_loss': '0.5276', 'train/train_time_sec': '5.9907', 'train/train_steps_per_sec': '5469.8364', 'collect/temperature': '0.1000', 'collect/collect_time_sec': '1050.5193', 'collect/collect_steps_per_sec': '0.9748', 'buffer/populated': '204191.0000', 'buffer/has_reward': '1046457.0000', 'buffer/fullness_pct': '19.4732', 'perf/epoch_time_sec': '1056.5154'}\n",
      "Temperature: 0.1\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 9: {'l2_reg': '1.0742', 'loss': '2.6513', 'policy_entropy': '0.9690', 'policy_loss': '1.0509', 'value_loss': '0.5262', 'train/train_time_sec': '6.0139', 'train/train_steps_per_sec': '5448.6650', 'collect/temperature': '0.1000', 'collect/collect_time_sec': '1057.2646', 'collect/collect_steps_per_sec': '0.9685', 'buffer/populated': '226870.0000', 'buffer/has_reward': '1046569.0000', 'buffer/fullness_pct': '21.6360', 'perf/epoch_time_sec': '1063.2839'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 10: {'l2_reg': '1.0696', 'loss': '2.6521', 'policy_entropy': '0.9723', 'policy_loss': '1.0472', 'value_loss': '0.5353', 'train/train_time_sec': '6.1741', 'train/train_steps_per_sec': '5307.2930', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1061.7132', 'collect/collect_steps_per_sec': '0.9645', 'buffer/populated': '249566.0000', 'buffer/has_reward': '1045938.0000', 'buffer/fullness_pct': '23.8005', 'perf/epoch_time_sec': '1067.8928'}\n",
      "Testing\n",
      "Epoch 10: {'hit2_eval_avg_outcome': '-0.2000', 'perf/test_hit2_eval_time_sec': '293.3165'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 11: {'l2_reg': '1.0651', 'loss': '2.6346', 'policy_entropy': '0.9748', 'policy_loss': '1.0468', 'value_loss': '0.5227', 'train/train_time_sec': '6.0335', 'train/train_steps_per_sec': '5431.0205', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1059.2639', 'collect/collect_steps_per_sec': '0.9667', 'buffer/populated': '272187.0000', 'buffer/has_reward': '1047198.0000', 'buffer/fullness_pct': '25.9578', 'perf/epoch_time_sec': '1065.3030'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 15: {'l2_reg': '1.0478', 'loss': '2.6118', 'policy_entropy': '0.9778', 'policy_loss': '1.0350', 'value_loss': '0.5290', 'train/train_time_sec': '6.0024', 'train/train_steps_per_sec': '5459.1499', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1050.0041', 'collect/collect_steps_per_sec': '0.9752', 'buffer/populated': '362725.0000', 'buffer/has_reward': '1045911.0000', 'buffer/fullness_pct': '34.5922', 'perf/epoch_time_sec': '1056.0119'}\n",
      "Testing\n",
      "Epoch 15: {'hit2_eval_avg_outcome': '0.1000', 'perf/test_hit2_eval_time_sec': '283.7317'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 17: {'l2_reg': '1.0395', 'loss': '2.5900', 'policy_entropy': '0.9716', 'policy_loss': '1.0238', 'value_loss': '0.5266', 'train/train_time_sec': '6.0662', 'train/train_steps_per_sec': '5401.7598', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1049.9910', 'collect/collect_steps_per_sec': '0.9752', 'buffer/populated': '408032.0000', 'buffer/has_reward': '1046253.0000', 'buffer/fullness_pct': '38.9130', 'perf/epoch_time_sec': '1056.0629'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 18: {'l2_reg': '1.0355', 'loss': '2.5826', 'policy_entropy': '0.9769', 'policy_loss': '1.0248', 'value_loss': '0.5223', 'train/train_time_sec': '6.0562', 'train/train_steps_per_sec': '5410.6665', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1049.8054', 'collect/collect_steps_per_sec': '0.9754', 'buffer/populated': '430653.0000', 'buffer/has_reward': '1046561.0000', 'buffer/fullness_pct': '41.0703', 'perf/epoch_time_sec': '1055.8670'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 19: {'l2_reg': '1.0315', 'loss': '2.5816', 'policy_entropy': '0.9751', 'policy_loss': '1.0268', 'value_loss': '0.5234', 'train/train_time_sec': '6.0980', 'train/train_steps_per_sec': '5373.5781', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1052.3095', 'collect/collect_steps_per_sec': '0.9731', 'buffer/populated': '453272.0000', 'buffer/has_reward': '1046944.0000', 'buffer/fullness_pct': '43.2274', 'perf/epoch_time_sec': '1058.4129'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 20: {'l2_reg': '1.0276', 'loss': '2.5720', 'policy_entropy': '0.9709', 'policy_loss': '1.0189', 'value_loss': '0.5256', 'train/train_time_sec': '5.9775', 'train/train_steps_per_sec': '5481.8618', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1044.4379', 'collect/collect_steps_per_sec': '0.9804', 'buffer/populated': '475902.0000', 'buffer/has_reward': '1045857.0000', 'buffer/fullness_pct': '45.3856', 'perf/epoch_time_sec': '1050.4210'}\n",
      "Testing\n",
      "Epoch 20: {'hit2_eval_avg_outcome': '0.2667', 'perf/test_hit2_eval_time_sec': '288.4911'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 22: {'l2_reg': '1.0199', 'loss': '2.5693', 'policy_entropy': '0.9766', 'policy_loss': '1.0201', 'value_loss': '0.5293', 'train/train_time_sec': '5.9878', 'train/train_steps_per_sec': '5472.4487', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1042.7067', 'collect/collect_steps_per_sec': '0.9821', 'buffer/populated': '521061.0000', 'buffer/has_reward': '1046666.0000', 'buffer/fullness_pct': '49.6922', 'perf/epoch_time_sec': '1048.7001'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 23: {'l2_reg': '1.0162', 'loss': '2.5530', 'policy_entropy': '0.9754', 'policy_loss': '1.0184', 'value_loss': '0.5185', 'train/train_time_sec': '6.2059', 'train/train_steps_per_sec': '5280.1494', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1042.2457', 'collect/collect_steps_per_sec': '0.9825', 'buffer/populated': '543648.0000', 'buffer/has_reward': '1046446.0000', 'buffer/fullness_pct': '51.8463', 'perf/epoch_time_sec': '1048.4571'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 24: {'l2_reg': '1.0125', 'loss': '2.5433', 'policy_entropy': '0.9744', 'policy_loss': '1.0166', 'value_loss': '0.5142', 'train/train_time_sec': '5.9809', 'train/train_steps_per_sec': '5478.7510', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1040.7166', 'collect/collect_steps_per_sec': '0.9839', 'buffer/populated': '566287.0000', 'buffer/has_reward': '1045919.0000', 'buffer/fullness_pct': '54.0053', 'perf/epoch_time_sec': '1046.7030'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 25: {'l2_reg': '1.0089', 'loss': '2.5651', 'policy_entropy': '0.9821', 'policy_loss': '1.0199', 'value_loss': '0.5364', 'train/train_time_sec': '6.1227', 'train/train_steps_per_sec': '5351.9033', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1044.9880', 'collect/collect_steps_per_sec': '0.9799', 'buffer/populated': '588885.0000', 'buffer/has_reward': '1046706.0000', 'buffer/fullness_pct': '56.1604', 'perf/epoch_time_sec': '1051.1164'}\n",
      "Testing\n",
      "Training\n",
      "Training Done\n",
      "Epoch 26: {'l2_reg': '1.0053', 'loss': '2.5509', 'policy_entropy': '0.9753', 'policy_loss': '1.0140', 'value_loss': '0.5316', 'train/train_time_sec': '6.0137', 'train/train_steps_per_sec': '5448.8574', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1053.1799', 'collect/collect_steps_per_sec': '0.9723', 'buffer/populated': '611485.0000', 'buffer/has_reward': '1046643.0000', 'buffer/fullness_pct': '58.3158', 'perf/epoch_time_sec': '1059.1989'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 28: {'l2_reg': '0.9983', 'loss': '2.5242', 'policy_entropy': '0.9717', 'policy_loss': '1.0052', 'value_loss': '0.5206', 'train/train_time_sec': '5.9681', 'train/train_steps_per_sec': '5490.5410', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1054.7039', 'collect/collect_steps_per_sec': '0.9709', 'buffer/populated': '656654.0000', 'buffer/has_reward': '1046626.0000', 'buffer/fullness_pct': '62.6234', 'perf/epoch_time_sec': '1060.6775'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 29: {'l2_reg': '0.9949', 'loss': '2.5152', 'policy_entropy': '0.9688', 'policy_loss': '1.0039', 'value_loss': '0.5164', 'train/train_time_sec': '5.9957', 'train/train_steps_per_sec': '5465.2480', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1059.2574', 'collect/collect_steps_per_sec': '0.9667', 'buffer/populated': '679288.0000', 'buffer/has_reward': '1046677.0000', 'buffer/fullness_pct': '64.7820', 'perf/epoch_time_sec': '1065.2584'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 31: {'l2_reg': '0.9882', 'loss': '2.5015', 'policy_entropy': '0.9641', 'policy_loss': '0.9971', 'value_loss': '0.5161', 'train/train_time_sec': '6.0639', 'train/train_steps_per_sec': '5403.7993', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1042.5911', 'collect/collect_steps_per_sec': '0.9822', 'buffer/populated': '724484.0000', 'buffer/has_reward': '1046503.0000', 'buffer/fullness_pct': '69.0922', 'perf/epoch_time_sec': '1048.6606'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 32: {'l2_reg': '0.9849', 'loss': '2.5065', 'policy_entropy': '0.9712', 'policy_loss': '1.0034', 'value_loss': '0.5182', 'train/train_time_sec': '6.0675', 'train/train_steps_per_sec': '5400.5508', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1045.5602', 'collect/collect_steps_per_sec': '0.9794', 'buffer/populated': '747040.0000', 'buffer/has_reward': '1046419.0000', 'buffer/fullness_pct': '71.2433', 'perf/epoch_time_sec': '1051.6331'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 33: {'l2_reg': '0.9817', 'loss': '2.5048', 'policy_entropy': '0.9695', 'policy_loss': '1.0019', 'value_loss': '0.5212', 'train/train_time_sec': '6.0996', 'train/train_steps_per_sec': '5372.1685', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1048.3528', 'collect/collect_steps_per_sec': '0.9768', 'buffer/populated': '769614.0000', 'buffer/has_reward': '1046652.0000', 'buffer/fullness_pct': '73.3961', 'perf/epoch_time_sec': '1054.4578'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 34: {'l2_reg': '0.9785', 'loss': '2.4837', 'policy_entropy': '0.9676', 'policy_loss': '0.9981', 'value_loss': '0.5072', 'train/train_time_sec': '6.0055', 'train/train_steps_per_sec': '5456.3354', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1040.2628', 'collect/collect_steps_per_sec': '0.9844', 'buffer/populated': '792100.0000', 'buffer/has_reward': '1046983.0000', 'buffer/fullness_pct': '75.5405', 'perf/epoch_time_sec': '1046.2737'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 35: {'l2_reg': '0.9753', 'loss': '2.4820', 'policy_entropy': '0.9617', 'policy_loss': '0.9937', 'value_loss': '0.5130', 'train/train_time_sec': '6.1112', 'train/train_steps_per_sec': '5361.9897', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1045.1500', 'collect/collect_steps_per_sec': '0.9798', 'buffer/populated': '814659.0000', 'buffer/has_reward': '1046644.0000', 'buffer/fullness_pct': '77.6919', 'perf/epoch_time_sec': '1051.2668'}\n",
      "Testing\n",
      "Epoch 35: {'hit2_eval_avg_outcome': '0.3333', 'perf/test_hit2_eval_time_sec': '276.4728'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 36: {'l2_reg': '0.9722', 'loss': '2.4964', 'policy_entropy': '0.9704', 'policy_loss': '1.0011', 'value_loss': '0.5232', 'train/train_time_sec': '6.0664', 'train/train_steps_per_sec': '5401.5923', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1020.7251', 'collect/collect_steps_per_sec': '1.0032', 'buffer/populated': '837213.0000', 'buffer/has_reward': '1046832.0000', 'buffer/fullness_pct': '79.8428', 'perf/epoch_time_sec': '1026.7969'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 38: {'l2_reg': '0.9660', 'loss': '2.4912', 'policy_entropy': '0.9733', 'policy_loss': '1.0008', 'value_loss': '0.5244', 'train/train_time_sec': '5.9933', 'train/train_steps_per_sec': '5467.4551', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1025.5486', 'collect/collect_steps_per_sec': '0.9985', 'buffer/populated': '882337.0000', 'buffer/has_reward': '1046834.0000', 'buffer/fullness_pct': '84.1462', 'perf/epoch_time_sec': '1031.5476'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 58: {'l2_reg': '0.9110', 'loss': '2.4340', 'policy_entropy': '0.9734', 'policy_loss': '0.9934', 'value_loss': '0.5296', 'train/train_time_sec': '6.0458', 'train/train_steps_per_sec': '5419.9448', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1037.4466', 'collect/collect_steps_per_sec': '0.9870', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1046730.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1043.4981'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 59: {'l2_reg': '0.9085', 'loss': '2.4291', 'policy_entropy': '0.9688', 'policy_loss': '0.9870', 'value_loss': '0.5335', 'train/train_time_sec': '5.9451', 'train/train_steps_per_sec': '5511.7568', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1041.9478', 'collect/collect_steps_per_sec': '0.9828', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1046251.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1047.8985'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 60: {'l2_reg': '0.9060', 'loss': '2.4294', 'policy_entropy': '0.9633', 'policy_loss': '0.9830', 'value_loss': '0.5405', 'train/train_time_sec': '6.1949', 'train/train_steps_per_sec': '5289.4951', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1037.5843', 'collect/collect_steps_per_sec': '0.9869', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1046499.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1043.7846'}\n",
      "Testing\n",
      "Training\n",
      "Training Done\n",
      "Epoch 71: {'l2_reg': '0.8793', 'loss': '2.3939', 'policy_entropy': '0.9691', 'policy_loss': '0.9841', 'value_loss': '0.5305', 'train/train_time_sec': '5.9477', 'train/train_steps_per_sec': '5509.3315', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1042.9840', 'collect/collect_steps_per_sec': '0.9818', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1046345.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1048.9373'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 79: {'l2_reg': '0.8608', 'loss': '2.3784', 'policy_entropy': '0.9711', 'policy_loss': '0.9839', 'value_loss': '0.5337', 'train/train_time_sec': '6.1015', 'train/train_steps_per_sec': '5370.4512', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1037.0196', 'collect/collect_steps_per_sec': '0.9874', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1046116.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1043.1266'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 80: {'l2_reg': '0.8585', 'loss': '2.3939', 'policy_entropy': '0.9756', 'policy_loss': '0.9912', 'value_loss': '0.5441', 'train/train_time_sec': '6.1227', 'train/train_steps_per_sec': '5351.8921', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1032.7483', 'collect/collect_steps_per_sec': '0.9915', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1046436.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1038.8767'}\n",
      "Testing\n",
      "Epoch 80: {'hit2_eval_avg_outcome': '0.5333', 'perf/test_hit2_eval_time_sec': '272.0027'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 81: {'l2_reg': '0.8563', 'loss': '2.3870', 'policy_entropy': '0.9726', 'policy_loss': '0.9831', 'value_loss': '0.5477', 'train/train_time_sec': '6.1687', 'train/train_steps_per_sec': '5311.9409', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1031.5517', 'collect/collect_steps_per_sec': '0.9927', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1046628.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1037.7259'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 84: {'l2_reg': '0.8495', 'loss': '2.3736', 'policy_entropy': '0.9665', 'policy_loss': '0.9801', 'value_loss': '0.5440', 'train/train_time_sec': '6.2110', 'train/train_steps_per_sec': '5275.8159', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1020.5727', 'collect/collect_steps_per_sec': '1.0034', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1047026.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1026.7889'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 85: {'l2_reg': '0.8473', 'loss': '2.3733', 'policy_entropy': '0.9738', 'policy_loss': '0.9849', 'value_loss': '0.5411', 'train/train_time_sec': '6.0813', 'train/train_steps_per_sec': '5388.3032', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1025.2399', 'collect/collect_steps_per_sec': '0.9988', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1046692.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1031.3268'}\n",
      "Testing\n",
      "Epoch 85: {'hit2_eval_avg_outcome': '0.1000', 'perf/test_hit2_eval_time_sec': '284.0312'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 86: {'l2_reg': '0.8451', 'loss': '2.3626', 'policy_entropy': '0.9735', 'policy_loss': '0.9865', 'value_loss': '0.5309', 'train/train_time_sec': '5.9565', 'train/train_steps_per_sec': '5501.2100', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1042.3432', 'collect/collect_steps_per_sec': '0.9824', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1046280.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1048.3050'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 93: {'l2_reg': '0.8300', 'loss': '2.3274', 'policy_entropy': '0.9648', 'policy_loss': '0.9744', 'value_loss': '0.5230', 'train/train_time_sec': '5.9955', 'train/train_steps_per_sec': '5465.4775', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1022.5822', 'collect/collect_steps_per_sec': '1.0014', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1046558.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1028.5832'}\n",
      "Temperature: 0.0\n",
      "Collecting self-play games\n",
      "Training\n",
      "Training Done\n",
      "Epoch 99: {'l2_reg': '0.8175', 'loss': '2.3280', 'policy_entropy': '0.9679', 'policy_loss': '0.9768', 'value_loss': '0.5338', 'train/train_time_sec': '6.0212', 'train/train_steps_per_sec': '5442.1465', 'collect/temperature': '0.0000', 'collect/collect_time_sec': '1021.3385', 'collect/collect_steps_per_sec': '1.0026', 'buffer/populated': '1048576.0000', 'buffer/has_reward': '1046594.0000', 'buffer/fullness_pct': '100.0000', 'perf/epoch_time_sec': '1027.3654'}\n",
      "Training loop completed successfully.\n"
     ]
    }
   ],
   "source": [
    "#output = trainer.train_loop(seed=0, num_epochs=50, eval_every=5)\n",
    "\n",
    "\n",
    "output = None\n",
    "# Update temperature for each epoch\n",
    "adjust_temp_count=5\n",
    "\n",
    "trainer.set_temp_fn(get_temperature)\n",
    "\n",
    "output = trainer.train_loop(seed=42, num_epochs=100, eval_every=5)\n",
    "\n",
    "print(\"Training loop completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and GIFs generated will appear in the same directory as this notebook, and also on your `wandb` dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbov4",
   "language": "python",
   "name": "turbov4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
