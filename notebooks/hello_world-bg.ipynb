{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World, TurboZero Backgammon üèÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`turbozero` provides a vectorized implementation of AlphaZero. \n",
    "\n",
    "In a nutshell, this means we can massively speed up training, by collecting many self-play games and running Monte Carlo Tree Search in parallel across one or more GPUs!\n",
    "\n",
    "As the user, you just need to provide:\n",
    "* environment dynamics functions (step and init) that adhere to the TurboZero spec\n",
    "* a conversion function for environment state -> neural net input\n",
    "* and a few hyperparameters!\n",
    "\n",
    "TurboZero takes care of the rest. üòÄ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Follow the instructions in the repo readme to properly install dependencies and set up your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "In order to take advantage of the batched implementation of AlphaZero, we need to pair it with a vectorized environment.\n",
    "\n",
    "Fortunately, there are many great vectorized RL environment libraries, one I like in particular is [pgx](https://github.com/sotetsuk/pgx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "True\n",
      "156\n",
      "[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/m61xxbzx54q1_974vvvdp7940000gn/T/ipykernel_80671/2151107264.py:4: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
      "  print(xla_bridge.get_backend().platform)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<svg baseProfile=\"full\" height=\"375.0\" version=\"1.1\" width=\"450.0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><rect fill=\"white\" height=\"425\" width=\"575\" x=\"0\" y=\"0\" /><g transform=\"scale(1.0)\"><rect fill=\"white\" height=\"375\" width=\"450\" x=\"0\" y=\"0\" /><g transform=\"translate(12.5,12.5)\"><polygon fill=\"white\" points=\"0,0 25,0 12.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"25,0 50,0 37.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"50,0 75,0 62.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"75,0 100,0 87.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"100,0 125,0 112.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"125,0 150,0 137.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"175,0 200,0 187.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"200,0 225,0 212.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"225,0 250,0 237.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"250,0 275,0 262.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"275,0 300,0 287.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"300,0 325,0 312.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"0,350 25,350 12.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"25,350 50,350 37.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"50,350 75,350 62.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"75,350 100,350 87.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"100,350 125,350 112.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"125,350 150,350 137.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"175,350 200,350 187.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"200,350 225,350 212.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"225,350 250,350 237.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"250,350 275,350 262.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"275,350 300,350 287.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"300,350 325,350 312.5,200\" stroke=\"gray\" /><rect fill=\"none\" height=\"350\" stroke=\"black\" width=\"325\" x=\"0\" y=\"0\" /><rect fill=\"none\" height=\"350\" stroke=\"black\" width=\"25\" x=\"150\" y=\"0\" /><circle cx=\"312.5\" cy=\"337.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"312.5\" cy=\"312.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"337.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"312.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"287.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"262.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"237.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"337.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"312.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"287.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"337.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"312.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"287.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"262.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"237.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"12.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"37.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"62.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"87.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"112.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"12.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"37.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"62.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"12.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"37.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"62.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"87.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"112.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"312.5\" cy=\"12.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"312.5\" cy=\"37.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><rect fill=\"white\" height=\"50\" stroke=\"black\" width=\"100\" x=\"325\" y=\"0\" /><circle cx=\"350\" cy=\"25\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><text fill=\"black\" font-family=\"serif\" font-size=\"34px\" x=\"365.0\" y=\"35.0\">√ó0</text><rect fill=\"white\" height=\"50\" stroke=\"black\" width=\"100\" x=\"325\" y=\"300\" /><circle cx=\"350\" cy=\"325\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><text fill=\"black\" font-family=\"serif\" font-size=\"34px\" x=\"365.0\" y=\"335.0\">√ó0</text><text fill=\"black\" font-family=\"sans serif\" font-size=\"44px\" x=\"337.5\" y=\"187.5\">‚öÖ</text><text fill=\"black\" font-family=\"sans serif\" font-size=\"44px\" x=\"370.0\" y=\"187.5\">‚öÅ</text></g></g></svg>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "#jax.config.update('jax_platform_name', 'gpu')\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)\n",
    "\n",
    "import pgx\n",
    "import pgx.backgammon as bg\n",
    "\n",
    "\n",
    "env = bg.Backgammon(simple_doubles=True)\n",
    "print(env.simple_doubles)\n",
    "print(env.num_actions)\n",
    "print(env.stochastic_action_probs)\n",
    "\n",
    "# create key\n",
    "key = jax.random.PRNGKey(0)\n",
    "state = env.init(key)\n",
    "state.to_svg()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Dynamics\n",
    "\n",
    "Turbozero needs to interface with the environment in order to build search trees and collect self-play episodes.\n",
    "\n",
    "We can define this interface with the following functions:\n",
    "* `env_step_fn`: given an environment state and an action, return the new environment state \n",
    "```python\n",
    "    EnvStepFn = Callable[[chex.ArrayTree, int], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "* `env_init_fn`: given a key, initialize and reutrn a new environment state\n",
    "```python\n",
    "    EnvInitFn = Callable[[chex.PRNGKey], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "Fortunately, environment libraries implement these for us! We just need to extract a few key pieces of information \n",
    "from the environment state so that we can match the TurboZero specification. We store this in a StepMetadata object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34m@\u001b[0m\u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mStepMetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Metadata for a step in the environment.\u001b[0m\n",
      "\u001b[0;34m    - `rewards`: rewards received by the players\u001b[0m\n",
      "\u001b[0;34m    - `action_mask`: mask of valid actions\u001b[0m\n",
      "\u001b[0;34m    - `terminated`: whether the environment is terminated\u001b[0m\n",
      "\u001b[0;34m    - `cur_player_id`: current player id\u001b[0m\n",
      "\u001b[0;34m    - `step`: step number\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrewards\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maction_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mterminated\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcur_player_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from core.types import StepMetadata\n",
    "%psource StepMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `rewards` stores the rewards emitted for each player for the given timestep\n",
    "* `action_mask` is a mask across all possible actions, where legal actions are set to `True`, and invalid/illegal actions are set to `False`\n",
    "* `terminated` True if the environment is terminated/completed\n",
    "* `cur_player_id`: id of the current player\n",
    "* `step`: step number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the environment interface for `Backgammon` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "def step_fn(state, action):\n",
    "    \"\"\"Handle regular backgammon moves.\n",
    "    \n",
    "    Args:\n",
    "        state: Current environment state\n",
    "        action: The move to make (index into legal_action_mask)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (new_state, metadata)\n",
    "    \"\"\"\n",
    "\n",
    "    new_state = jax.lax.cond(\n",
    "        state.is_stochastic,\n",
    "        env.stochastic_step,\n",
    "        env.step,\n",
    "        state,\n",
    "        action\n",
    "    )\n",
    "\n",
    "    return new_state, StepMetadata(\n",
    "        rewards=new_state.rewards,\n",
    "        action_mask=new_state.legal_action_mask,\n",
    "        terminated=new_state.terminated,\n",
    "        cur_player_id=new_state.current_player,\n",
    "        step=new_state._step_count\n",
    "    )\n",
    "                       \n",
    "def init_fn(key):\n",
    "    \"\"\"Initialize a new backgammon game.\n",
    "    \n",
    "    Args:\n",
    "        key: Random key for initialization\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (initial_state, metadata)\n",
    "    \"\"\"\n",
    "    state = env.init(key)\n",
    "\n",
    "    return state, StepMetadata(\n",
    "        rewards=state.rewards,\n",
    "        action_mask=state.legal_action_mask,\n",
    "        terminated=state.terminated,\n",
    "        cur_player_id=state.current_player,\n",
    "        step=state._step_count\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Next, we'll need to define the architecture of the neural network \n",
    "\n",
    "A simple implementation of the residual neural network used in the _AlphaZero_ paper is included for your convenience. \n",
    "\n",
    "You can implement your own architecture using `flax.linen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.networks.azresnet import AZResnetConfig, AZResnet\n",
    "\n",
    "resnet = AZResnet(AZResnetConfig(\n",
    "    policy_head_out_size=env.num_actions,\n",
    "    num_blocks=4,\n",
    "    num_channels=32,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to convert our environment's state into something our neural network can take as input (i.e. structured data -> Array). `pgx` conveniently includes this in `state.observation`, but for other environments you may need to perform the conversion yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_nn_input(state):\n",
    "    return state.observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Next, we can initialize our evaluator, AlphaZero, which takes the following parameters:\n",
    "\n",
    "* `eval_fn`: function used to evaluate a leaf node (returns a policy and value)\n",
    "* `num_iterations`: number of MCTS iterations to run before returning the final policy\n",
    "* `max_nodes`: maximum capacity of search tree\n",
    "* `branching_factor`: branching factor of search tree == policy_size\n",
    "* `action_selector`: the algorithm used to select an action to take at any given search node, choose between:\n",
    "    * `PUCTSelector`: AlphaZero action selection algorithm\n",
    "    * `MuZeroPUCTSelector`: MuZero action selection algorithm\n",
    "    * or write your own! :)\n",
    "\n",
    "There are also a few other optional parameters, a few of the important ones are:\n",
    "* `temperature`: temperature applied to move probabilities prior to sampling (0.0 == argmax, ->inf == completely random sampling). I reccommend setting this to 1.0 for training (default) and 0.0 for evaluation.\n",
    "* `dirichlet_alpha`: magnitude of Dirichlet noise to add to root policy (default 0.3). Generally, the more actions are possible in a game, the smaller this value should be. \n",
    "* `dirichlet_epsilon`: proportion of root policy composed of Dirichlet noise (default 0.25)\n",
    "\n",
    "\n",
    "We use `make_nn_eval_fn` to create a leaf evaluation function that uses our neural network to generate a policy and a value for the given state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.evaluators.alphazero import AlphaZero\n",
    "from core.evaluators.evaluation_fns import make_nn_eval_fn\n",
    "from core.evaluators.mcts.action_selection import PUCTSelector\n",
    "from core.evaluators.mcts.stochastic_mcts import StochasticMCTS\n",
    "import jax.numpy as jnp\n",
    "from core.types import StepMetadata\n",
    "\n",
    "# alphazero can take an arbirary search `backend`\n",
    "# here we use classic MCTS\n",
    "az_evaluator = StochasticMCTS(\n",
    "    eval_fn = make_nn_eval_fn(resnet, state_to_nn_input),\n",
    "    stochastic_action_probs = env.stochastic_action_probs,\n",
    "    num_iterations = 32,\n",
    "    max_nodes = 40,\n",
    "    branching_factor = env.num_actions,\n",
    "    action_selector = PUCTSelector(),\n",
    "    temperature = 1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a separate evaluator with different parameters to use for testing purposes. We'll give this one a larger budget (num_iterations), and set the temperature to zero so it always chooses the most-visited action after search is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_evaluator_test = StochasticMCTS(\n",
    "            eval_fn = make_nn_eval_fn(resnet, state_to_nn_input),\n",
    "            stochastic_action_probs = env.stochastic_action_probs,\n",
    "            action_selector = PUCTSelector(),\n",
    "            branching_factor = env.num_actions,\n",
    "            max_nodes = 80,\n",
    "            num_iterations = 64,\n",
    "            temperature = 0.0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use similar ideas to write a greedy baseline evaluation function, one that doesn't use a neural network at all!\n",
    "\n",
    "Instead, it simply counts the number of tiles for the active player and compares it to the number of tiles controlled by the other player, so the value is higher for states where the active player controls more tiles than the other player.\n",
    "\n",
    "Using similar techniques as before, we can create another AlphaZero evaluator to test against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.evaluators.evaluation_fns import make_nn_eval_fn_no_params_callable\n",
    "import chex\n",
    "\n",
    "def backgammon_pip_count_eval(state: chex.ArrayTree):\n",
    "    \"\"\"\n",
    "    Calculates a value heuristic based on pip count difference for Backgammon\n",
    "    and returns uniform policy logits over legal actions.\n",
    "\n",
    "    Args:\n",
    "        state: The pgx.backgammon.State object.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[chex.Array, chex.Array]: (policy_logits, value)\n",
    "    \"\"\"\n",
    "    # --- Pip Count Calculation ---\n",
    "    # board: (26,) array: -15 to +15 checkers.\n",
    "    # Indices 1-24 are points. 0=P0 bar, 25=P1 bar.\n",
    "    # Off-board checkers are implicitly handled (pip count becomes 0).\n",
    "    board = state._board\n",
    "\n",
    "    # Checkers for each player on points 1-24\n",
    "    loc_player_0 = jnp.maximum(0, board[1:25])  # P0 checkers (positive values)\n",
    "    loc_player_1 = jnp.maximum(0, -board[1:25]) # P1 checkers (negative values)\n",
    "\n",
    "    points = jnp.arange(1, 25) # Point numbers 1 to 24\n",
    "\n",
    "    # Pip counts for checkers on points 1-24\n",
    "    # Player 0: wants small point numbers (points * count)\n",
    "    # Player 1: wants large point numbers ( (25 - points) * count )\n",
    "    pip_player_0 = jnp.sum(loc_player_0 * points)\n",
    "    pip_player_1 = jnp.sum(loc_player_1 * (25 - points))\n",
    "\n",
    "    # Add pips for checkers on the bar\n",
    "    # P0 bar (index 0): counts as 25 pips\n",
    "    # P1 bar (index 25): counts as 25 pips\n",
    "    pip_player_0 += jnp.maximum(0, board[0]) * 25\n",
    "    pip_player_1 += jnp.maximum(0, -board[25]) * 25\n",
    "\n",
    "    # --- Value Calculation ---\n",
    "    # Lower pip count is better. Value = normalized difference.\n",
    "    # We want high value if *current player* has the lower pip count.\n",
    "    total_pips = pip_player_0 + pip_player_1 + 1e-6 # Avoid division by zero\n",
    "\n",
    "    # Value from Player 0's perspective (higher if P0 is winning)\n",
    "    value_p0_perspective = (pip_player_1 - pip_player_0) / total_pips\n",
    "\n",
    "    # Adjust value based on current player\n",
    "    value = jnp.where(state.current_player == 0, value_p0_perspective, -value_p0_perspective)\n",
    "\n",
    "    # --- Policy Logits ---\n",
    "    # Uniform policy over legal actions for greedy baseline\n",
    "    policy_logits = jnp.where(state.legal_action_mask, 0.0, -jnp.inf)\n",
    "\n",
    "    return policy_logits, jnp.array(value)\n",
    "\n",
    "\n",
    "greedy_az = StochasticMCTS(\n",
    "    eval_fn = backgammon_pip_count_eval,\n",
    "    stochastic_action_probs = env.stochastic_action_probs,\n",
    "    num_iterations = 64,\n",
    "    max_nodes = 80,\n",
    "    branching_factor = env.num_actions,\n",
    "    action_selector = PUCTSelector(),\n",
    "    temperature = 0.0,\n",
    "            \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory Buffer\n",
    "\n",
    "Next, we'll initialize a replay memory buffer to hold selfplay trajectories that we can sample from during training. This actually just defines an interface, the buffer state itself will be initialized and managed internally.\n",
    "\n",
    "The replay buffer is batched, it retains a buffer of trajectories across a batch dimension. We specify a `capacity`: the amount of samples stored in a single buffer. The total capacity of the entire replay buffer is then `batch_size * capacity`, where `batch_size` is the number of environments/self-play games being run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.memory.replay_memory import EpisodeReplayBuffer\n",
    "\n",
    "replay_memory = EpisodeReplayBuffer(capacity=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation (Optional)\n",
    "\n",
    "During self-play, we allow for any number of custom data augmentation functions, in order to create more training samples. \n",
    "\n",
    "In RL, it's sometimes common to take advantage of rotations or symmetries in order to generate additional training examples. \n",
    "\n",
    "In Othello, we could simply consider rotating the board to generate a new training example, we will need to be careful to update our policy as well.\n",
    "\n",
    "In order to implement a data augmentation function, we must follow `DataTransformFn`:\n",
    "```python\n",
    "# (policy mask, policy weights, environment state) -> (transformed policy mask, transformed policy weights, transformed environment state)\n",
    "DataTransformFn = Callable[[chex.Array, chex.Array, chex.ArrayTree], Tuple[chex.Array, chex.Array, chex.ArrayTree]]\n",
    "```\n",
    "\n",
    "We create rotational transform functions for rotating 90, 180, 270 degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rot_transform_fn(amnt: int):\n",
    "    def rot_transform_fn(mask, policy, state):\n",
    "        action_ids = jnp.arange(65) # 65 total actions, but only rotate the first 64! (65th is always do nothing action)\n",
    "        # we only use state.observation, no need to update the rest of the state fields\n",
    "        new_obs = jnp.rot90(state.observation, amnt, axes=(-3,-2))\n",
    "        # map action ids to new action ids\n",
    "        idxs = jnp.arange(64).reshape(8,8) # rotate first 64 actions\n",
    "        new_idxs = jnp.rot90(idxs, amnt, axes=(0, 1)).flatten()\n",
    "        action_ids = action_ids.at[:64].set(new_idxs)\n",
    "        # get new mask and policy\n",
    "        new_mask = mask[...,action_ids]\n",
    "        new_policy = policy[...,action_ids]\n",
    "        return new_mask, new_policy, state.replace(observation=new_obs)\n",
    "\n",
    "    return rot_transform_fn\n",
    "\n",
    "# make transform fns for rotating 90, 180, 270 degrees\n",
    "transforms = [make_rot_transform_fn(i) for i in range(1,4)]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering\n",
    "We can optionally provide a `render_fn` that will record games played by our model against one of the baselines and save it as a `.gif`.\n",
    "\n",
    "I've included a helper fn that takes care of this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function depends upon cairoSVG, which itself depends upon `cairo`, which you'll need to install on your system.\n",
    "\n",
    "On Ubuntu, this can be done with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewrobertson/.pyenv/versions/3.12.5/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "! apt-get update && apt-get -y install libcairo2-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're on another OS, consult https://www.cairographics.org/download/ for installation guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from core.testing.utils import render_pgx_2p\n",
    "render_fn = partial(render_pgx_2p, p1_label='Black', p2_label='White', duration=900)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Initialization\n",
    "Now that we have all the proper pieces defined, we are ready to initialize a Trainer and start training!\n",
    "\n",
    "The `Trainer` takes many parameters, so let's walk through them all:\n",
    "* `batch_size`: # of parallel environments used to collect self-play games\n",
    "* `train_batch_size`: size of minibatch used during training step\n",
    "* `warmup_steps`: # of steps (per batch) to collect via self-play prior to entering the training loop. This is used to populate the replay memory with some initial samples\n",
    "* `collection_steps_per_epoch`: # of steps (per batch) to collect via self-play per epoch\n",
    "* `train_steps_per_epoch`: # of train steps per epoch\n",
    "* `nn`: neural network (`linen.Module`)\n",
    "* `loss_fn`: loss function used for training, we use a provided default loss which implements the loss function used in the `AlphaZero` paper\n",
    "* `optimizer`: an `optax` optimizer used for training\n",
    "* `evaluator`: the `Evaluator` to use during self-play, we initialized ours using `AlphaZero(MCTS)`\n",
    "* `memory_buffer`: the memory buffer used to store samples from self-play games, we  initialized ours using `EpisodeReplayBuffer`\n",
    "* `max_episode_steps`: maximum number of steps/turns to allow before truncating an episode\n",
    "* `env_step_fn`: environment step function (we defined ours above)\n",
    "* `env_init_fn`: environment init function (we defined ours above)\n",
    "* `state_to_nn_input_fn`: function to convert environment state to nn input (we defined ours above)\n",
    "* `testers`: any number of `Tester`s, used to evaluate a given model and take their own parameters. We'll use the two evaluators defined above to initialize two Testers.\n",
    "* `evaluator_test`: (Optional) Evaluator used within Testers. By default used `evaluator`, but sometimes you may want to test with a larger MCTS iteration budget for example, or a different move sampling temperature\n",
    "* `data_transform_fns`: (optional) list of data transform functions to apply to self-play experiences (e.g. rotation, reflection, etc.)\n",
    "* `extract_model_params_fn`: (Optional) in special cases we need to define how to extract all model parameters from a flax `TrainState`. The default function handles BatchNorm, but if another special-case technique applied across batches is used (e.g. Dropout) we would need to define a function to extract the appropriate parameters. You usually won't need to define this!\n",
    "* `wandb_project_name`: (Optional) Weights and Biases project name. You will be prompted to login if a name is provided. If a name is provided, a run will be initialized and loss and other metrics will be logged to the given wandb project.\n",
    "* `ckpt_dir`: (Optional) directory to store checkpoints in, by default this is set to `/tmp/turbozero_checkpoints`\n",
    "* `max_checkpoints`: (Optional) maximum number of most-recent checkpoints to retain (default: 2)\n",
    "* `num_devices`: (Optional) number of hardware accelerators (GPUs/TPUs) to use. If not given, all available hardware accelerators are used\n",
    "* `wandb_run`: (Optional) continues from an initialized `wandb` run if provided, otherwise a new one is initialized\n",
    "* `extra_wandb_config`: (Optional) any extra metadata to store in the `wandb` run config\n",
    "\n",
    "A training epoch is comprised of M collection steps, followed by N training steps sampling minibatches from replay memory. Optionally, any number of Testers evaluate the current model. At the end of each epoch, a checkpoint is saved.\n",
    "\n",
    "If you are using one or more GPUs (reccommended), TurboZero by default will run on all your available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from core.testing.two_player_baseline import TwoPlayerBaseline\n",
    "from core.training.loss_fns import az_default_loss_fn\n",
    "from core.training.train import Trainer\n",
    "import optax\n",
    "\n",
    "trainer = Trainer(\n",
    "    batch_size = 1024,\n",
    "    train_batch_size = 4096,\n",
    "    warmup_steps = 0,\n",
    "    collection_steps_per_epoch = 256,\n",
    "    train_steps_per_epoch = 64,\n",
    "    nn = resnet,\n",
    "    loss_fn = partial(az_default_loss_fn, l2_reg_lambda = 0.0),\n",
    "    optimizer = optax.adam(1e-3),\n",
    "    evaluator = az_evaluator,\n",
    "    memory_buffer = replay_memory,\n",
    "    max_episode_steps = 80,\n",
    "    env_step_fn = step_fn,\n",
    "    env_init_fn = init_fn,\n",
    "    state_to_nn_input_fn=state_to_nn_input,\n",
    "    testers = [\n",
    "        #TwoPlayerBaseline(num_episodes=128, baseline_evaluator=baseline_az, render_fn=render_fn, render_dir='.', name='pretrained'),\n",
    "        TwoPlayerBaseline(num_episodes=128, baseline_evaluator=greedy_az, render_fn=render_fn, render_dir='.', name='greedy'),\n",
    "    ],\n",
    "    evaluator_test = az_evaluator_test,\n",
    "    data_transform_fns=transforms\n",
    "    # wandb_project_name = 'turbozero-othello' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now all that's left to do is to kick off the training loop! We need to pass an initial seed for reproducibility, and the number of epochs to run for!\n",
    "\n",
    "If you've set up `wandb`, you can track metrics via plots in the run dashboard. Metrics will also be printed to the console. \n",
    "\n",
    "IMPORTANT: The first epoch will not execute quickly! This is because there is significant overhead in JAX compilation (nearly all of the training loop is JIT-compiled). This will cause the first epoch to run very slowly, as JIT-compiled functions are traced and compiled the first time they are run. Expect epochs after the first to execute much more quickly. Typically, GPU utilization will also be low/zero during this period.\n",
    "\n",
    "It's also worth mentioning that the hyperparameters in this notebook are just here for example purposes. Regardless of the task, they will need to be tuned according to the characteristics of the environment as well as your available hardware and time/cost constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (0,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/turbozero/core/training/train.py:603\u001b[0m, in \u001b[0;36mTrainer.train_loop\u001b[0;34m(self, seed, num_epochs, eval_every, initial_state)\u001b[0m\n\u001b[1;32m    601\u001b[0m init_key, key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[1;32m    602\u001b[0m init_keys \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mtile(init_key[\u001b[38;5;28;01mNone\u001b[39;00m], (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_devices, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 603\u001b[0m train_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_train_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_model_params_fn(train_state)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# initialize tester states\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/github/turbozero/core/training/train.py:235\u001b[0m, in \u001b[0;36mTrainer.init_train_state\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    233\u001b[0m sample_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_to_nn_input_fn(sample_env_state)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# initialize nn parameters\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m params \u001b[38;5;241m=\u001b[39m variables[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# handle batchnorm\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/github/turbozero/core/networks/azresnet.py:42\u001b[0m, in \u001b[0;36mAZResnet.__call__\u001b[0;34m(self, x, train)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, train: \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# initial conv layer\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm(use_running_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m train)(x)\n\u001b[1;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/github/turbozero/.venv/lib/python3.12/site-packages/flax/linen/linear.py:663\u001b[0m, in \u001b[0;36m_Conv.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    662\u001b[0m     conv_general_dilated \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mconv_general_dilated\n\u001b[0;32m--> 663\u001b[0m   y \u001b[38;5;241m=\u001b[39m \u001b[43mconv_general_dilated\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_lax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_dilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdimension_numbers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_group_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_group_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m   y \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mconv_general_dilated_local(\n\u001b[1;32m    676\u001b[0m     lhs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    677\u001b[0m     rhs\u001b[38;5;241m=\u001b[39mkernel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    684\u001b[0m     precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision,\n\u001b[1;32m    685\u001b[0m   )\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/github/turbozero/.venv/lib/python3.12/site-packages/jax/_src/lax/lax.py:5861\u001b[0m, in \u001b[0;36m_ceil_divide\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m   5860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ceil_divide\u001b[39m(x1, x2):\n\u001b[0;32m-> 5861\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloor_divide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (0,) (2,) "
     ]
    }
   ],
   "source": [
    "output = trainer.train_loop(seed=0, num_epochs=100, eval_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and GIFs generated will appear in the same directory as this notebook, and also on your `wandb` dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
