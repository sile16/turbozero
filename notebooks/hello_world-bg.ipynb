{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World, TurboZero Backgammon ðŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`turbozero` provides a vectorized implementation of AlphaZero. \n",
    "\n",
    "In a nutshell, this means we can massively speed up training, by collecting many self-play games and running Monte Carlo Tree Search in parallel across one or more GPUs!\n",
    "\n",
    "As the user, you just need to provide:\n",
    "* environment dynamics functions (step and init) that adhere to the TurboZero spec\n",
    "* a conversion function for environment state -> neural net input\n",
    "* and a few hyperparameters!\n",
    "\n",
    "TurboZero takes care of the rest. ðŸ˜€ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Follow the instructions in the repo readme to properly install dependencies and set up your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "In order to take advantage of the batched implementation of AlphaZero, we need to pair it with a vectorized environment.\n",
    "\n",
    "Fortunately, there are many great vectorized RL environment libraries, one I like in particular is [pgx](https://github.com/sotetsuk/pgx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax Version:  0.5.3\n",
      "Default backend: gpu\n",
      "0.5.3\n",
      "True\n",
      "156\n",
      "[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg baseProfile=\"full\" height=\"375.0\" version=\"1.1\" width=\"450.0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><rect fill=\"white\" height=\"425\" width=\"575\" x=\"0\" y=\"0\" /><g transform=\"scale(1.0)\"><rect fill=\"white\" height=\"375\" width=\"450\" x=\"0\" y=\"0\" /><g transform=\"translate(12.5,12.5)\"><polygon fill=\"white\" points=\"0,0 25,0 12.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"25,0 50,0 37.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"50,0 75,0 62.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"75,0 100,0 87.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"100,0 125,0 112.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"125,0 150,0 137.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"175,0 200,0 187.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"200,0 225,0 212.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"225,0 250,0 237.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"250,0 275,0 262.5,150\" stroke=\"gray\" /><polygon fill=\"white\" points=\"275,0 300,0 287.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"300,0 325,0 312.5,150\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"0,350 25,350 12.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"25,350 50,350 37.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"50,350 75,350 62.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"75,350 100,350 87.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"100,350 125,350 112.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"125,350 150,350 137.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"175,350 200,350 187.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"200,350 225,350 212.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"225,350 250,350 237.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"250,350 275,350 262.5,200\" stroke=\"gray\" /><polygon fill=\"gray\" points=\"275,350 300,350 287.5,200\" stroke=\"gray\" /><polygon fill=\"white\" points=\"300,350 325,350 312.5,200\" stroke=\"gray\" /><rect fill=\"none\" height=\"350\" stroke=\"black\" width=\"325\" x=\"0\" y=\"0\" /><rect fill=\"none\" height=\"350\" stroke=\"black\" width=\"25\" x=\"150\" y=\"0\" /><circle cx=\"312.5\" cy=\"337.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"312.5\" cy=\"312.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"337.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"312.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"287.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"262.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"237.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"337.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"312.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"287.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"337.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"312.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"287.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"262.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"237.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"12.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"37.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"62.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"87.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"12.5\" cy=\"112.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"12.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"37.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"112.5\" cy=\"62.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"12.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"37.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"62.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"87.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"187.5\" cy=\"112.5\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><circle cx=\"312.5\" cy=\"12.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><circle cx=\"312.5\" cy=\"37.5\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><rect fill=\"white\" height=\"50\" stroke=\"black\" width=\"100\" x=\"325\" y=\"0\" /><circle cx=\"350\" cy=\"25\" fill=\"black\" r=\"12.5\" stroke=\"black\" /><text fill=\"black\" font-family=\"serif\" font-size=\"34px\" x=\"365.0\" y=\"35.0\">Ã—0</text><rect fill=\"white\" height=\"50\" stroke=\"black\" width=\"100\" x=\"325\" y=\"300\" /><circle cx=\"350\" cy=\"325\" fill=\"white\" r=\"12.5\" stroke=\"black\" /><text fill=\"black\" font-family=\"serif\" font-size=\"34px\" x=\"365.0\" y=\"335.0\">Ã—0</text><text fill=\"black\" font-family=\"sans serif\" font-size=\"44px\" x=\"337.5\" y=\"187.5\">âš€</text><text fill=\"black\" font-family=\"sans serif\" font-size=\"44px\" x=\"370.0\" y=\"187.5\">âšƒ</text></g></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jax\n",
    "print(\"Jax Version: \",jax.__version__)\n",
    "#jax.config.update('jax_platform_name', 'gpu')\n",
    "from jax.lib import xla_bridge\n",
    "from prompt_toolkit import HTML\n",
    "print(\"Default backend:\", jax.default_backend())\n",
    "\n",
    "import pgx\n",
    "import pgx.backgammon as bg\n",
    "\n",
    "print(jax.__version__)\n",
    "\n",
    "\n",
    "env = bg.Backgammon(simple_doubles=True)\n",
    "print(env.simple_doubles)\n",
    "print(env.num_actions)\n",
    "print(env.stochastic_action_probs)\n",
    "\n",
    "# create key\n",
    "key = jax.random.PRNGKey(0)\n",
    "state = env.init(key)\n",
    "from IPython.display import HTML\n",
    "display(HTML(state.to_svg()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Dynamics\n",
    "\n",
    "Turbozero needs to interface with the environment in order to build search trees and collect self-play episodes.\n",
    "\n",
    "We can define this interface with the following functions:\n",
    "* `env_step_fn`: given an environment state and an action, return the new environment state \n",
    "```python\n",
    "    EnvStepFn = Callable[[chex.ArrayTree, int], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "* `env_init_fn`: given a key, initialize and reutrn a new environment state\n",
    "```python\n",
    "    EnvInitFn = Callable[[chex.PRNGKey], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "Fortunately, environment libraries implement these for us! We just need to extract a few key pieces of information \n",
    "from the environment state so that we can match the TurboZero specification. We store this in a StepMetadata object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.types import StepMetadata\n",
    "%psource StepMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `rewards` stores the rewards emitted for each player for the given timestep\n",
    "* `action_mask` is a mask across all possible actions, where legal actions are set to `True`, and invalid/illegal actions are set to `False`\n",
    "* `terminated` True if the environment is terminated/completed\n",
    "* `cur_player_id`: id of the current player\n",
    "* `step`: step number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the environment interface for `Backgammon` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chex\n",
    "from typing import Tuple\n",
    "\n",
    "def step_fn(state: bg.State, action: int, key: chex.PRNGKey) -> Tuple[bg.State, StepMetadata]:\n",
    "    \"\"\"Combined step function for backgammon environment that handles both deterministic and stochastic actions.\"\"\"\n",
    "    # print(f\"[DEBUG-BG_STEP-{time.time()}] Called with state (stochastic={state.is_stochastic}), action={action}\") # Optional debug\n",
    "\n",
    "    # Handle stochastic vs deterministic branches\n",
    "    def stochastic_branch(operand):\n",
    "        s, a, _ = operand # state, action, key (key ignored for stochastic step)\n",
    "        # Use env instance captured by closure (assuming env is accessible in this scope)\n",
    "        return env.stochastic_step(s, a)\n",
    "\n",
    "    def deterministic_branch(operand):\n",
    "        s, a, k = operand # state, action, key\n",
    "        # Use env instance captured by closure\n",
    "        return env.step(s, a, k)\n",
    "\n",
    "    # Use conditional to route to the appropriate branch\n",
    "    # The key is only needed for the deterministic branch\n",
    "    new_state = jax.lax.cond(\n",
    "        state.is_stochastic,\n",
    "        stochastic_branch,\n",
    "        deterministic_branch,\n",
    "        (state, action, key) # Pass all required operands\n",
    "    )\n",
    "\n",
    "    # Create standard metadata\n",
    "    metadata = StepMetadata(\n",
    "        rewards=new_state.rewards,\n",
    "        action_mask=new_state.legal_action_mask,\n",
    "        terminated=new_state.terminated,\n",
    "        cur_player_id=new_state.current_player,\n",
    "        step=new_state._step_count\n",
    "    )\n",
    "\n",
    "    return new_state, metadata\n",
    "\n",
    "def init_fn(key):\n",
    "    \"\"\"Initializes a new environment state.\"\"\"\n",
    "    state = env.init(key)\n",
    "    # No need to force non-stochastic, let the environment handle it\n",
    "    return state, StepMetadata(\n",
    "        rewards=state.rewards,\n",
    "        action_mask=state.legal_action_mask,\n",
    "        terminated=state.terminated,\n",
    "        cur_player_id=state.current_player,\n",
    "        step=state._step_count\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Next, we'll need to define the architecture of the neural network \n",
    "\n",
    "A simple implementation of the residual neural network used in the _AlphaZero_ paper is included for your convenience. \n",
    "\n",
    "You can implement your own architecture using `flax.linen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.networks.mlp import MLPConfig, MLP\n",
    "\n",
    "# Replace the resnet with an MLP network\n",
    "mlp_network = MLP(MLPConfig(\n",
    "    hidden_dims=[128, 128, 64],  # Adjust layer sizes as needed\n",
    "    policy_head_out_size=env.num_actions,\n",
    "    value_head_out_size=1\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to convert our environment's state into something our neural network can take as input (i.e. structured data -> Array). `pgx` conveniently includes this in `state.observation`, but for other environments you may need to perform the conversion yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_nn_input(state):\n",
    "    return state.observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Next, we can initialize our evaluator, AlphaZero, which takes the following parameters:\n",
    "\n",
    "* `eval_fn`: function used to evaluate a leaf node (returns a policy and value)\n",
    "* `num_iterations`: number of MCTS iterations to run before returning the final policy\n",
    "* `max_nodes`: maximum capacity of search tree\n",
    "* `branching_factor`: branching factor of search tree == policy_size\n",
    "* `action_selector`: the algorithm used to select an action to take at any given search node, choose between:\n",
    "    * `PUCTSelector`: AlphaZero action selection algorithm\n",
    "    * `MuZeroPUCTSelector`: MuZero action selection algorithm\n",
    "    * or write your own! :)\n",
    "\n",
    "There are also a few other optional parameters, a few of the important ones are:\n",
    "* `temperature`: temperature applied to move probabilities prior to sampling (0.0 == argmax, ->inf == completely random sampling). I reccommend setting this to 1.0 for training (default) and 0.0 for evaluation.\n",
    "* `dirichlet_alpha`: magnitude of Dirichlet noise to add to root policy (default 0.3). Generally, the more actions are possible in a game, the smaller this value should be. \n",
    "* `dirichlet_epsilon`: proportion of root policy composed of Dirichlet noise (default 0.25)\n",
    "\n",
    "\n",
    "We use `make_nn_eval_fn` to create a leaf evaluation function that uses our neural network to generate a policy and a value for the given state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from core.evaluators.evaluation_fns import make_nn_eval_fn\n",
    "from core.evaluators.mcts.action_selection import PUCTSelector\n",
    "from core.evaluators.mcts.stochastic_mcts import StochasticMCTS\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Training evaluator: StochasticMCTS using NN\n",
    "evaluator = StochasticMCTS(   #Explores new moves\n",
    "    eval_fn=make_nn_eval_fn(mlp_network, state_to_nn_input),\n",
    "    stochastic_action_probs=env.stochastic_action_probs,\n",
    "    num_iterations=200,  \n",
    "    max_nodes=300,      \n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector=PUCTSelector(),\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a separate evaluator with different parameters to use for testing purposes. We'll give this one a larger budget (num_iterations), and set the temperature to zero so it always chooses the most-visited action after search is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluator_test = StochasticMCTS(   #Use optimized moves, temperature=0.0\n",
    "    eval_fn=make_nn_eval_fn(mlp_network, state_to_nn_input),\n",
    "    stochastic_action_probs=env.stochastic_action_probs,\n",
    "    num_iterations=200,  # Very few iterations\n",
    "    max_nodes=300,      # Very small tree\n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector=PUCTSelector(),\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use similar ideas to write a greedy baseline evaluation function, one that doesn't use a neural network at all!\n",
    "\n",
    "Instead, it simply counts the number of tiles for the active player and compares it to the number of tiles controlled by the other player, so the value is higher for states where the active player controls more tiles than the other player.\n",
    "\n",
    "Using similar techniques as before, we can create another AlphaZero evaluator to test against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.evaluators.evaluation_fns import make_nn_eval_fn_no_params_callable\n",
    "import chex\n",
    "\n",
    "# --- Pip Count Eval Fn (for test evaluator) ---\n",
    "def backgammon_pip_count_eval(state: chex.ArrayTree, params: chex.ArrayTree, key: chex.PRNGKey):\n",
    "    \"\"\"Calculates value based on pip count difference. Ignores params/key.\"\"\"\n",
    "    board = state._board\n",
    "    loc_player_0 = jnp.maximum(0, board[1:25])\n",
    "    loc_player_1 = jnp.maximum(0, -board[1:25])\n",
    "    points = jnp.arange(1, 25)\n",
    "    pip_player_0 = jnp.sum(loc_player_0 * points)\n",
    "    pip_player_1 = jnp.sum(loc_player_1 * (25 - points))\n",
    "    pip_player_0 += jnp.maximum(0, board[0]) * 25\n",
    "    pip_player_1 += jnp.maximum(0, -board[25]) * 25\n",
    "    total_pips = pip_player_0 + pip_player_1 + 1e-6\n",
    "    value_p0_perspective = (pip_player_1 - pip_player_0) / total_pips\n",
    "    value = jnp.where(state.current_player == 0, value_p0_perspective, -value_p0_perspective)\n",
    "    # Uniform policy over legal actions for greedy baseline\n",
    "    policy_logits = jnp.where(state.legal_action_mask, 0.0, -jnp.inf)\n",
    "    return policy_logits, jnp.array(value)\n",
    "\n",
    "\n",
    "# Test evaluator: Regular MCTS using pip count\n",
    "pip_count_mcts_evaluator_test = StochasticMCTS(  # optimizes for moves\n",
    "    eval_fn=backgammon_pip_count_eval, # Use pip count eval fn\n",
    "    stochastic_action_probs=env.stochastic_action_probs,\n",
    "    num_iterations=30, # Give it slightly more iterations maybe\n",
    "    max_nodes=100,\n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector=PUCTSelector(),\n",
    "    temperature=0.0 # Deterministic action selection for testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory Buffer\n",
    "\n",
    "Next, we'll initialize a replay memory buffer to hold selfplay trajectories that we can sample from during training. This actually just defines an interface, the buffer state itself will be initialized and managed internally.\n",
    "\n",
    "The replay buffer is batched, it retains a buffer of trajectories across a batch dimension. We specify a `capacity`: the amount of samples stored in a single buffer. The total capacity of the entire replay buffer is then `batch_size * capacity`, where `batch_size` is the number of environments/self-play games being run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.memory.replay_memory import EpisodeReplayBuffer\n",
    "\n",
    "replay_memory = EpisodeReplayBuffer(capacity=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering\n",
    "We can optionally provide a `render_fn` that will record games played by our model against one of the baselines and save it as a `.gif`.\n",
    "\n",
    "I've included a helper fn that takes care of this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function depends upon cairoSVG, which itself depends upon `cairo`, which you'll need to install on your system.\n",
    "\n",
    "On Ubuntu, this can be done with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 0%\r",
      "\r",
      "Reading package lists... 100%\r",
      "\r",
      "Reading package lists... Done\r",
      "\r\n",
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\r\n",
      "E: Unable to lock directory /var/lib/apt/lists/\r\n",
      "W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\r\n",
      "W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sile/.pyenv/versions/3.12.8/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "! apt-get update && apt-get -y install libcairo2-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're on another OS, consult https://www.cairographics.org/download/ for installation guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from core.testing.utils import render_pgx_2p\n",
    "render_fn = partial(render_pgx_2p, p1_label='Black', p2_label='White', duration=900)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Initialization\n",
    "Now that we have all the proper pieces defined, we are ready to initialize a Trainer and start training!\n",
    "\n",
    "The `Trainer` takes many parameters, so let's walk through them all:\n",
    "* `batch_size`: # of parallel environments used to collect self-play games\n",
    "* `train_batch_size`: size of minibatch used during training step\n",
    "* `warmup_steps`: # of steps (per batch) to collect via self-play prior to entering the training loop. This is used to populate the replay memory with some initial samples\n",
    "* `collection_steps_per_epoch`: # of steps (per batch) to collect via self-play per epoch\n",
    "* `train_steps_per_epoch`: # of train steps per epoch\n",
    "* `nn`: neural network (`linen.Module`)\n",
    "* `loss_fn`: loss function used for training, we use a provided default loss which implements the loss function used in the `AlphaZero` paper\n",
    "* `optimizer`: an `optax` optimizer used for training\n",
    "* `evaluator`: the `Evaluator` to use during self-play, we initialized ours using `AlphaZero(MCTS)`\n",
    "* `memory_buffer`: the memory buffer used to store samples from self-play games, we  initialized ours using `EpisodeReplayBuffer`\n",
    "* `max_episode_steps`: maximum number of steps/turns to allow before truncating an episode\n",
    "* `env_step_fn`: environment step function (we defined ours above)\n",
    "* `env_init_fn`: environment init function (we defined ours above)\n",
    "* `state_to_nn_input_fn`: function to convert environment state to nn input (we defined ours above)\n",
    "* `testers`: any number of `Tester`s, used to evaluate a given model and take their own parameters. We'll use the two evaluators defined above to initialize two Testers.\n",
    "* `evaluator_test`: (Optional) Evaluator used within Testers. By default used `evaluator`, but sometimes you may want to test with a larger MCTS iteration budget for example, or a different move sampling temperature\n",
    "* `data_transform_fns`: (optional) list of data transform functions to apply to self-play experiences (e.g. rotation, reflection, etc.)\n",
    "* `extract_model_params_fn`: (Optional) in special cases we need to define how to extract all model parameters from a flax `TrainState`. The default function handles BatchNorm, but if another special-case technique applied across batches is used (e.g. Dropout) we would need to define a function to extract the appropriate parameters. You usually won't need to define this!\n",
    "* `wandb_project_name`: (Optional) Weights and Biases project name. You will be prompted to login if a name is provided. If a name is provided, a run will be initialized and loss and other metrics will be logged to the given wandb project.\n",
    "* `ckpt_dir`: (Optional) directory to store checkpoints in, by default this is set to `/tmp/turbozero_checkpoints`\n",
    "* `max_checkpoints`: (Optional) maximum number of most-recent checkpoints to retain (default: 2)\n",
    "* `num_devices`: (Optional) number of hardware accelerators (GPUs/TPUs) to use. If not given, all available hardware accelerators are used\n",
    "* `wandb_run`: (Optional) continues from an initialized `wandb` run if provided, otherwise a new one is initialized\n",
    "* `extra_wandb_config`: (Optional) any extra metadata to store in the `wandb` run config\n",
    "\n",
    "A training epoch is comprised of M collection steps, followed by N training steps sampling minibatches from replay memory. Optionally, any number of Testers evaluate the current model. At the end of each epoch, a checkpoint is saved.\n",
    "\n",
    "If you are using one or more GPUs (reccommended), TurboZero by default will run on all your available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msile16\u001b[0m (\u001b[33msile16-self\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from core.testing.two_player_baseline import TwoPlayerBaseline\n",
    "from core.training.loss_fns import az_default_loss_fn\n",
    "from core.training.stochastic_train import StochasticTrainer\n",
    "from core.training.train import Trainer\n",
    "import optax\n",
    "\n",
    "trainer = StochasticTrainer(\n",
    "    batch_size=128,      # Minimal batch size\n",
    "    train_batch_size=50,\n",
    "    warmup_steps=0,\n",
    "    collection_steps_per_epoch=512,  # Just 2 collection step\n",
    "    train_steps_per_epoch=512,       # Just 2 training step\n",
    "    nn=mlp_network,\n",
    "    loss_fn=partial(az_default_loss_fn, l2_reg_lambda=0.0),\n",
    "    optimizer=optax.adam(1e-4),\n",
    "    # Use the stochastic evaluator for training\n",
    "    evaluator=evaluator, \n",
    "    memory_buffer=replay_memory,\n",
    "    max_episode_steps=500,  # Super short episodes\n",
    "    env_step_fn=step_fn,\n",
    "    env_init_fn=init_fn,\n",
    "    state_to_nn_input_fn=state_to_nn_input,\n",
    "    ckpt_dir = \"/tmp/ckpts\",\n",
    "    testers=[\n",
    "        # Use our custom BackgammonTwoPlayerBaseline\n",
    "        TwoPlayerBaseline(\n",
    "            num_episodes=20,\n",
    "            baseline_evaluator=pip_count_mcts_evaluator_test,\n",
    "            #render_fn=render_fn,\n",
    "            #render_dir='training_eval/pip_count_baseline',\n",
    "            name='pip_count_baseline'\n",
    "        )\n",
    "    ],\n",
    "    # Use the pip count MCTS evaluator for testing\n",
    "    evaluator_test=evaluator_test, \n",
    "    data_transform_fns=[],  # No data transforms as requested\n",
    "    wandb_project_name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now all that's left to do is to kick off the training loop! We need to pass an initial seed for reproducibility, and the number of epochs to run for!\n",
    "\n",
    "If you've set up `wandb`, you can track metrics via plots in the run dashboard. Metrics will also be printed to the console. \n",
    "\n",
    "IMPORTANT: The first epoch will not execute quickly! This is because there is significant overhead in JAX compilation (nearly all of the training loop is JIT-compiled). This will cause the first epoch to run very slowly, as JIT-compiled functions are traced and compiled the first time they are run. Expect epochs after the first to execute much more quickly. Typically, GPU utilization will also be low/zero during this period.\n",
    "\n",
    "It's also worth mentioning that the hyperparameters in this notebook are just here for example purposes. Regardless of the task, they will need to be tuned according to the characteristics of the environment as well as your available hardware and time/cost constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: {'l2_reg': '0.0000', 'loss': '2.2401', 'policy_entropy': '0.9424', 'policy_loss': '0.9889', 'value_loss': '1.2512', 'buffer/size': '50970.0000', 'buffer/valid_samples': '118200.0000', 'buffer/fullness_pct': '39.8203', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.6890', 'perf/train_time_sec': '21.3884', 'perf/collect_steps_per_sec': '1.6427', 'perf/collect_game_steps_per_sec': '210.2609', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 0: {'pip_count_baseline_avg_outcome': '0.6500'}\n",
      "Epoch 1: {'l2_reg': '0.0000', 'loss': '2.1494', 'policy_entropy': '0.9680', 'policy_loss': '1.0241', 'value_loss': '1.1253', 'buffer/size': '102050.0000', 'buffer/valid_samples': '118450.0000', 'buffer/fullness_pct': '79.7266', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '307.0906', 'perf/train_time_sec': '14.6771', 'perf/collect_steps_per_sec': '1.6673', 'perf/collect_game_steps_per_sec': '213.4093', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 2: {'l2_reg': '0.0000', 'loss': '2.0727', 'policy_entropy': '0.9725', 'policy_loss': '1.0210', 'value_loss': '1.0517', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118005.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.9259', 'perf/train_time_sec': '14.6332', 'perf/collect_steps_per_sec': '1.6414', 'perf/collect_game_steps_per_sec': '210.1012', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 3: {'l2_reg': '0.0000', 'loss': '2.0267', 'policy_entropy': '0.9734', 'policy_loss': '1.0145', 'value_loss': '1.0122', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118908.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '308.8728', 'perf/train_time_sec': '14.4165', 'perf/collect_steps_per_sec': '1.6576', 'perf/collect_game_steps_per_sec': '212.1779', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 4: {'l2_reg': '0.0000', 'loss': '1.9869', 'policy_entropy': '0.9727', 'policy_loss': '1.0004', 'value_loss': '0.9865', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118349.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '310.3038', 'perf/train_time_sec': '14.9684', 'perf/collect_steps_per_sec': '1.6500', 'perf/collect_game_steps_per_sec': '211.1995', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 5: {'l2_reg': '0.0000', 'loss': '1.9696', 'policy_entropy': '0.9854', 'policy_loss': '1.0087', 'value_loss': '0.9609', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119224.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '315.0026', 'perf/train_time_sec': '14.7921', 'perf/collect_steps_per_sec': '1.6254', 'perf/collect_game_steps_per_sec': '208.0491', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 5: {'pip_count_baseline_avg_outcome': '-0.1000'}\n",
      "Epoch 6: {'l2_reg': '0.0000', 'loss': '1.9176', 'policy_entropy': '0.9708', 'policy_loss': '0.9924', 'value_loss': '0.9252', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117051.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.5501', 'perf/train_time_sec': '14.4889', 'perf/collect_steps_per_sec': '1.6381', 'perf/collect_game_steps_per_sec': '209.6816', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 7: {'l2_reg': '0.0000', 'loss': '1.9063', 'policy_entropy': '0.9656', 'policy_loss': '0.9832', 'value_loss': '0.9232', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118564.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '322.1212', 'perf/train_time_sec': '14.4291', 'perf/collect_steps_per_sec': '1.5895', 'perf/collect_game_steps_per_sec': '203.4514', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 8: {'l2_reg': '0.0000', 'loss': '1.9170', 'policy_entropy': '0.9734', 'policy_loss': '0.9890', 'value_loss': '0.9280', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118782.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.5938', 'perf/train_time_sec': '14.6856', 'perf/collect_steps_per_sec': '1.6275', 'perf/collect_game_steps_per_sec': '208.3194', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 9: {'l2_reg': '0.0000', 'loss': '1.8920', 'policy_entropy': '0.9722', 'policy_loss': '0.9884', 'value_loss': '0.9036', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118667.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '321.1566', 'perf/train_time_sec': '14.8642', 'perf/collect_steps_per_sec': '1.5942', 'perf/collect_game_steps_per_sec': '204.0625', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 10: {'l2_reg': '0.0000', 'loss': '1.8654', 'policy_entropy': '0.9655', 'policy_loss': '0.9783', 'value_loss': '0.8870', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119492.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.6977', 'perf/train_time_sec': '14.8185', 'perf/collect_steps_per_sec': '1.6321', 'perf/collect_game_steps_per_sec': '208.9145', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 10: {'pip_count_baseline_avg_outcome': '-0.2000'}\n",
      "Epoch 11: {'l2_reg': '0.0000', 'loss': '1.9190', 'policy_entropy': '0.9718', 'policy_loss': '0.9898', 'value_loss': '0.9292', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118861.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.9620', 'perf/train_time_sec': '14.7476', 'perf/collect_steps_per_sec': '1.6412', 'perf/collect_game_steps_per_sec': '210.0768', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 12: {'l2_reg': '0.0000', 'loss': '1.9171', 'policy_entropy': '0.9744', 'policy_loss': '0.9911', 'value_loss': '0.9260', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117178.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.6497', 'perf/train_time_sec': '14.7177', 'perf/collect_steps_per_sec': '1.6324', 'perf/collect_game_steps_per_sec': '208.9465', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 13: {'l2_reg': '0.0000', 'loss': '1.8691', 'policy_entropy': '0.9778', 'policy_loss': '0.9911', 'value_loss': '0.8780', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117318.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '312.5448', 'perf/train_time_sec': '14.8783', 'perf/collect_steps_per_sec': '1.6382', 'perf/collect_game_steps_per_sec': '209.6852', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 14: {'l2_reg': '0.0000', 'loss': '1.8274', 'policy_entropy': '0.9769', 'policy_loss': '0.9901', 'value_loss': '0.8373', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117634.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.5659', 'perf/train_time_sec': '14.7172', 'perf/collect_steps_per_sec': '1.6328', 'perf/collect_game_steps_per_sec': '209.0023', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: {'l2_reg': '0.0000', 'loss': '1.8232', 'policy_entropy': '0.9831', 'policy_loss': '0.9926', 'value_loss': '0.8306', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118190.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.1035', 'perf/train_time_sec': '14.5774', 'perf/collect_steps_per_sec': '1.6405', 'perf/collect_game_steps_per_sec': '209.9816', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 15: {'pip_count_baseline_avg_outcome': '-0.4000'}\n",
      "Epoch 16: {'l2_reg': '0.0000', 'loss': '1.8789', 'policy_entropy': '0.9840', 'policy_loss': '0.9953', 'value_loss': '0.8836', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118201.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '309.8896', 'perf/train_time_sec': '14.7018', 'perf/collect_steps_per_sec': '1.6522', 'perf/collect_game_steps_per_sec': '211.4818', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 17: {'l2_reg': '0.0000', 'loss': '1.8556', 'policy_entropy': '0.9775', 'policy_loss': '0.9875', 'value_loss': '0.8682', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117055.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '312.8491', 'perf/train_time_sec': '14.6289', 'perf/collect_steps_per_sec': '1.6366', 'perf/collect_game_steps_per_sec': '209.4812', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 18: {'l2_reg': '0.0000', 'loss': '1.8743', 'policy_entropy': '0.9778', 'policy_loss': '0.9848', 'value_loss': '0.8894', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118524.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.9291', 'perf/train_time_sec': '14.6343', 'perf/collect_steps_per_sec': '1.6362', 'perf/collect_game_steps_per_sec': '209.4276', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 19: {'l2_reg': '0.0000', 'loss': '1.8805', 'policy_entropy': '0.9757', 'policy_loss': '0.9872', 'value_loss': '0.8933', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118002.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '317.3263', 'perf/train_time_sec': '14.5583', 'perf/collect_steps_per_sec': '1.6135', 'perf/collect_game_steps_per_sec': '206.5256', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 20: {'l2_reg': '0.0000', 'loss': '1.9002', 'policy_entropy': '0.9858', 'policy_loss': '0.9943', 'value_loss': '0.9058', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118286.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '307.5285', 'perf/train_time_sec': '14.7431', 'perf/collect_steps_per_sec': '1.6649', 'perf/collect_game_steps_per_sec': '213.1054', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 20: {'pip_count_baseline_avg_outcome': '-0.2000'}\n",
      "Epoch 21: {'l2_reg': '0.0000', 'loss': '1.8682', 'policy_entropy': '0.9768', 'policy_loss': '0.9859', 'value_loss': '0.8823', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118129.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '310.7409', 'perf/train_time_sec': '14.6254', 'perf/collect_steps_per_sec': '1.6477', 'perf/collect_game_steps_per_sec': '210.9024', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 22: {'l2_reg': '0.0000', 'loss': '1.8524', 'policy_entropy': '0.9820', 'policy_loss': '0.9888', 'value_loss': '0.8636', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119159.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '317.6819', 'perf/train_time_sec': '14.6790', 'perf/collect_steps_per_sec': '1.6117', 'perf/collect_game_steps_per_sec': '206.2944', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 23: {'l2_reg': '0.0000', 'loss': '1.8612', 'policy_entropy': '0.9781', 'policy_loss': '0.9855', 'value_loss': '0.8757', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118486.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.0173', 'perf/train_time_sec': '14.3843', 'perf/collect_steps_per_sec': '1.6409', 'perf/collect_game_steps_per_sec': '210.0396', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 24: {'l2_reg': '0.0000', 'loss': '1.8672', 'policy_entropy': '0.9864', 'policy_loss': '0.9931', 'value_loss': '0.8741', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118127.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '318.0959', 'perf/train_time_sec': '14.4552', 'perf/collect_steps_per_sec': '1.6096', 'perf/collect_game_steps_per_sec': '206.0259', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 25: {'l2_reg': '0.0000', 'loss': '1.8414', 'policy_entropy': '0.9725', 'policy_loss': '0.9762', 'value_loss': '0.8652', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117146.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.4393', 'perf/train_time_sec': '14.4205', 'perf/collect_steps_per_sec': '1.6387', 'perf/collect_game_steps_per_sec': '209.7560', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 25: {'pip_count_baseline_avg_outcome': '-0.3500'}\n",
      "Epoch 26: {'l2_reg': '0.0000', 'loss': '1.8497', 'policy_entropy': '0.9704', 'policy_loss': '0.9759', 'value_loss': '0.8739', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118729.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.5982', 'perf/train_time_sec': '14.3895', 'perf/collect_steps_per_sec': '1.6327', 'perf/collect_game_steps_per_sec': '208.9808', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 27: {'l2_reg': '0.0000', 'loss': '1.8878', 'policy_entropy': '0.9739', 'policy_loss': '0.9845', 'value_loss': '0.9033', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117918.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.7891', 'perf/train_time_sec': '14.4344', 'perf/collect_steps_per_sec': '1.6421', 'perf/collect_game_steps_per_sec': '210.1934', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 28: {'l2_reg': '0.0000', 'loss': '1.8211', 'policy_entropy': '0.9657', 'policy_loss': '0.9741', 'value_loss': '0.8470', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119184.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.4857', 'perf/train_time_sec': '14.5253', 'perf/collect_steps_per_sec': '1.6385', 'perf/collect_game_steps_per_sec': '209.7248', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 29: {'l2_reg': '0.0000', 'loss': '1.8304', 'policy_entropy': '0.9683', 'policy_loss': '0.9773', 'value_loss': '0.8531', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118160.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.9572', 'perf/train_time_sec': '14.4401', 'perf/collect_steps_per_sec': '1.6413', 'perf/collect_game_steps_per_sec': '210.0801', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: {'l2_reg': '0.0000', 'loss': '1.8467', 'policy_entropy': '0.9640', 'policy_loss': '0.9724', 'value_loss': '0.8743', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118194.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.4151', 'perf/train_time_sec': '14.4230', 'perf/collect_steps_per_sec': '1.6441', 'perf/collect_game_steps_per_sec': '210.4458', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 30: {'pip_count_baseline_avg_outcome': '-0.3000'}\n",
      "Epoch 31: {'l2_reg': '0.0000', 'loss': '1.8478', 'policy_entropy': '0.9720', 'policy_loss': '0.9798', 'value_loss': '0.8679', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118957.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '315.2819', 'perf/train_time_sec': '14.6543', 'perf/collect_steps_per_sec': '1.6239', 'perf/collect_game_steps_per_sec': '207.8648', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 32: {'l2_reg': '0.0000', 'loss': '1.7706', 'policy_entropy': '0.9657', 'policy_loss': '0.9703', 'value_loss': '0.8002', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117588.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.5504', 'perf/train_time_sec': '14.7181', 'perf/collect_steps_per_sec': '1.6434', 'perf/collect_game_steps_per_sec': '210.3544', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 33: {'l2_reg': '0.0000', 'loss': '1.7589', 'policy_entropy': '0.9619', 'policy_loss': '0.9673', 'value_loss': '0.7916', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117837.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '320.8697', 'perf/train_time_sec': '14.4559', 'perf/collect_steps_per_sec': '1.5957', 'perf/collect_game_steps_per_sec': '204.2449', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 34: {'l2_reg': '0.0000', 'loss': '1.7264', 'policy_entropy': '0.9692', 'policy_loss': '0.9723', 'value_loss': '0.7541', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118839.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.7881', 'perf/train_time_sec': '14.4527', 'perf/collect_steps_per_sec': '1.6317', 'perf/collect_game_steps_per_sec': '208.8543', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 35: {'l2_reg': '0.0000', 'loss': '1.7428', 'policy_entropy': '0.9667', 'policy_loss': '0.9739', 'value_loss': '0.7689', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117442.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '316.2382', 'perf/train_time_sec': '14.4835', 'perf/collect_steps_per_sec': '1.6190', 'perf/collect_game_steps_per_sec': '207.2362', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 35: {'pip_count_baseline_avg_outcome': '0.2500'}\n",
      "Epoch 36: {'l2_reg': '0.0000', 'loss': '1.7526', 'policy_entropy': '0.9668', 'policy_loss': '0.9709', 'value_loss': '0.7817', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117413.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.8382', 'perf/train_time_sec': '14.7497', 'perf/collect_steps_per_sec': '1.6419', 'perf/collect_game_steps_per_sec': '210.1603', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 37: {'l2_reg': '0.0000', 'loss': '1.7327', 'policy_entropy': '0.9630', 'policy_loss': '0.9695', 'value_loss': '0.7632', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118238.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '315.3885', 'perf/train_time_sec': '14.5169', 'perf/collect_steps_per_sec': '1.6234', 'perf/collect_game_steps_per_sec': '207.7945', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 38: {'l2_reg': '0.0000', 'loss': '1.7450', 'policy_entropy': '0.9646', 'policy_loss': '0.9685', 'value_loss': '0.7766', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118391.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '311.7146', 'perf/train_time_sec': '14.5741', 'perf/collect_steps_per_sec': '1.6425', 'perf/collect_game_steps_per_sec': '210.2436', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 39: {'l2_reg': '0.0000', 'loss': '1.7483', 'policy_entropy': '0.9566', 'policy_loss': '0.9590', 'value_loss': '0.7893', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117879.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '319.3586', 'perf/train_time_sec': '14.4099', 'perf/collect_steps_per_sec': '1.6032', 'perf/collect_game_steps_per_sec': '205.2113', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 40: {'l2_reg': '0.0000', 'loss': '1.7187', 'policy_entropy': '0.9497', 'policy_loss': '0.9561', 'value_loss': '0.7626', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118068.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '321.2113', 'perf/train_time_sec': '14.4349', 'perf/collect_steps_per_sec': '1.5940', 'perf/collect_game_steps_per_sec': '204.0277', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 40: {'pip_count_baseline_avg_outcome': '0.1500'}\n",
      "Epoch 41: {'l2_reg': '0.0000', 'loss': '1.7262', 'policy_entropy': '0.9620', 'policy_loss': '0.9669', 'value_loss': '0.7593', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118395.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.0828', 'perf/train_time_sec': '14.4262', 'perf/collect_steps_per_sec': '1.6301', 'perf/collect_game_steps_per_sec': '208.6584', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 42: {'l2_reg': '0.0000', 'loss': '1.7277', 'policy_entropy': '0.9590', 'policy_loss': '0.9625', 'value_loss': '0.7652', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117462.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '321.1549', 'perf/train_time_sec': '14.4794', 'perf/collect_steps_per_sec': '1.5942', 'perf/collect_game_steps_per_sec': '204.0635', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 43: {'l2_reg': '0.0000', 'loss': '1.7695', 'policy_entropy': '0.9625', 'policy_loss': '0.9671', 'value_loss': '0.8025', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118499.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.6559', 'perf/train_time_sec': '14.4625', 'perf/collect_steps_per_sec': '1.6376', 'perf/collect_game_steps_per_sec': '209.6106', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 44: {'l2_reg': '0.0000', 'loss': '1.7602', 'policy_entropy': '0.9554', 'policy_loss': '0.9587', 'value_loss': '0.8016', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118232.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.4826', 'perf/train_time_sec': '14.8221', 'perf/collect_steps_per_sec': '1.6281', 'perf/collect_game_steps_per_sec': '208.3931', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: {'l2_reg': '0.0000', 'loss': '1.8106', 'policy_entropy': '0.9595', 'policy_loss': '0.9642', 'value_loss': '0.8464', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118550.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.7660', 'perf/train_time_sec': '14.6716', 'perf/collect_steps_per_sec': '1.6266', 'perf/collect_game_steps_per_sec': '208.2054', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 45: {'pip_count_baseline_avg_outcome': '0.2000'}\n",
      "Epoch 46: {'l2_reg': '0.0000', 'loss': '1.7820', 'policy_entropy': '0.9554', 'policy_loss': '0.9604', 'value_loss': '0.8216', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119261.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '321.8299', 'perf/train_time_sec': '14.7894', 'perf/collect_steps_per_sec': '1.5909', 'perf/collect_game_steps_per_sec': '203.6355', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 47: {'l2_reg': '0.0000', 'loss': '1.6875', 'policy_entropy': '0.9466', 'policy_loss': '0.9520', 'value_loss': '0.7355', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118179.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '320.6976', 'perf/train_time_sec': '14.7844', 'perf/collect_steps_per_sec': '1.5965', 'perf/collect_game_steps_per_sec': '204.3545', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 48: {'l2_reg': '0.0000', 'loss': '1.6573', 'policy_entropy': '0.9472', 'policy_loss': '0.9478', 'value_loss': '0.7095', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117534.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.7695', 'perf/train_time_sec': '14.6669', 'perf/collect_steps_per_sec': '1.6370', 'perf/collect_game_steps_per_sec': '209.5345', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 49: {'l2_reg': '0.0000', 'loss': '1.6950', 'policy_entropy': '0.9425', 'policy_loss': '0.9437', 'value_loss': '0.7513', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119368.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.6216', 'perf/train_time_sec': '14.5614', 'perf/collect_steps_per_sec': '1.6378', 'perf/collect_game_steps_per_sec': '209.6336', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 50: {'l2_reg': '0.0000', 'loss': '1.7446', 'policy_entropy': '0.9519', 'policy_loss': '0.9541', 'value_loss': '0.7905', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118235.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.6180', 'perf/train_time_sec': '14.7090', 'perf/collect_steps_per_sec': '1.6326', 'perf/collect_game_steps_per_sec': '208.9676', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 50: {'pip_count_baseline_avg_outcome': '0.2500'}\n",
      "Epoch 51: {'l2_reg': '0.0000', 'loss': '1.7367', 'policy_entropy': '0.9493', 'policy_loss': '0.9565', 'value_loss': '0.7802', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117708.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.9070', 'perf/train_time_sec': '14.7845', 'perf/collect_steps_per_sec': '1.6311', 'perf/collect_game_steps_per_sec': '208.7752', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 52: {'l2_reg': '0.0000', 'loss': '1.7218', 'policy_entropy': '0.9547', 'policy_loss': '0.9630', 'value_loss': '0.7588', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118174.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.1344', 'perf/train_time_sec': '14.9227', 'perf/collect_steps_per_sec': '1.6351', 'perf/collect_game_steps_per_sec': '209.2903', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 53: {'l2_reg': '0.0000', 'loss': '1.7021', 'policy_entropy': '0.9632', 'policy_loss': '0.9699', 'value_loss': '0.7323', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118376.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '315.2935', 'perf/train_time_sec': '14.8033', 'perf/collect_steps_per_sec': '1.6239', 'perf/collect_game_steps_per_sec': '207.8571', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 54: {'l2_reg': '0.0000', 'loss': '1.7029', 'policy_entropy': '0.9601', 'policy_loss': '0.9641', 'value_loss': '0.7387', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118164.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '315.5418', 'perf/train_time_sec': '14.7300', 'perf/collect_steps_per_sec': '1.6226', 'perf/collect_game_steps_per_sec': '207.6936', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 55: {'l2_reg': '0.0000', 'loss': '1.7482', 'policy_entropy': '0.9678', 'policy_loss': '0.9733', 'value_loss': '0.7748', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118288.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '314.0245', 'perf/train_time_sec': '14.5279', 'perf/collect_steps_per_sec': '1.6304', 'perf/collect_game_steps_per_sec': '208.6971', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 55: {'pip_count_baseline_avg_outcome': '0.0500'}\n",
      "Epoch 56: {'l2_reg': '0.0000', 'loss': '1.7798', 'policy_entropy': '0.9623', 'policy_loss': '0.9688', 'value_loss': '0.8110', 'buffer/size': '128000.0000', 'buffer/valid_samples': '116882.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.7885', 'perf/train_time_sec': '14.6839', 'perf/collect_steps_per_sec': '1.6369', 'perf/collect_game_steps_per_sec': '209.5218', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 57: {'l2_reg': '0.0000', 'loss': '1.7782', 'policy_entropy': '0.9713', 'policy_loss': '0.9774', 'value_loss': '0.8008', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119588.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.5692', 'perf/train_time_sec': '14.7159', 'perf/collect_steps_per_sec': '1.6276', 'perf/collect_game_steps_per_sec': '208.3357', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 58: {'l2_reg': '0.0000', 'loss': '1.7924', 'policy_entropy': '0.9777', 'policy_loss': '0.9815', 'value_loss': '0.8109', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117417.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '305.2778', 'perf/train_time_sec': '14.7209', 'perf/collect_steps_per_sec': '1.6772', 'perf/collect_game_steps_per_sec': '214.6766', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 59: {'l2_reg': '0.0000', 'loss': '1.8097', 'policy_entropy': '0.9722', 'policy_loss': '0.9778', 'value_loss': '0.8320', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117265.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.9141', 'perf/train_time_sec': '14.4862', 'perf/collect_steps_per_sec': '1.6415', 'perf/collect_game_steps_per_sec': '210.1091', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: {'l2_reg': '0.0000', 'loss': '1.7532', 'policy_entropy': '0.9649', 'policy_loss': '0.9694', 'value_loss': '0.7838', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118267.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.3078', 'perf/train_time_sec': '14.5276', 'perf/collect_steps_per_sec': '1.6394', 'perf/collect_game_steps_per_sec': '209.8443', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 60: {'pip_count_baseline_avg_outcome': '-0.3500'}\n",
      "Epoch 61: {'l2_reg': '0.0000', 'loss': '1.7514', 'policy_entropy': '0.9640', 'policy_loss': '0.9663', 'value_loss': '0.7852', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117405.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.1055', 'perf/train_time_sec': '14.4274', 'perf/collect_steps_per_sec': '1.6457', 'perf/collect_game_steps_per_sec': '210.6552', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 62: {'l2_reg': '0.0000', 'loss': '1.7360', 'policy_entropy': '0.9559', 'policy_loss': '0.9612', 'value_loss': '0.7748', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117827.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.3505', 'perf/train_time_sec': '14.4457', 'perf/collect_steps_per_sec': '1.6392', 'perf/collect_game_steps_per_sec': '209.8156', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 63: {'l2_reg': '0.0000', 'loss': '1.7032', 'policy_entropy': '0.9492', 'policy_loss': '0.9529', 'value_loss': '0.7503', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117651.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '310.1488', 'perf/train_time_sec': '14.7174', 'perf/collect_steps_per_sec': '1.6508', 'perf/collect_game_steps_per_sec': '211.3050', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 64: {'l2_reg': '0.0000', 'loss': '1.7227', 'policy_entropy': '0.9538', 'policy_loss': '0.9602', 'value_loss': '0.7625', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119373.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.0338', 'perf/train_time_sec': '14.5304', 'perf/collect_steps_per_sec': '1.6461', 'perf/collect_game_steps_per_sec': '210.7038', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 65: {'l2_reg': '0.0000', 'loss': '1.7429', 'policy_entropy': '0.9527', 'policy_loss': '0.9533', 'value_loss': '0.7897', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118143.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '311.0785', 'perf/train_time_sec': '14.5790', 'perf/collect_steps_per_sec': '1.6459', 'perf/collect_game_steps_per_sec': '210.6735', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 65: {'pip_count_baseline_avg_outcome': '-0.1500'}\n",
      "Epoch 66: {'l2_reg': '0.0000', 'loss': '1.6876', 'policy_entropy': '0.9585', 'policy_loss': '0.9607', 'value_loss': '0.7269', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117699.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.3495', 'perf/train_time_sec': '14.6116', 'perf/collect_steps_per_sec': '1.6392', 'perf/collect_game_steps_per_sec': '209.8163', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 67: {'l2_reg': '0.0000', 'loss': '1.6744', 'policy_entropy': '0.9516', 'policy_loss': '0.9566', 'value_loss': '0.7179', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117990.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '308.5751', 'perf/train_time_sec': '14.7586', 'perf/collect_steps_per_sec': '1.6592', 'perf/collect_game_steps_per_sec': '212.3827', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 68: {'l2_reg': '0.0000', 'loss': '1.6859', 'policy_entropy': '0.9540', 'policy_loss': '0.9562', 'value_loss': '0.7298', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118780.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.3408', 'perf/train_time_sec': '14.6600', 'perf/collect_steps_per_sec': '1.6340', 'perf/collect_game_steps_per_sec': '209.1525', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 69: {'l2_reg': '0.0000', 'loss': '1.6828', 'policy_entropy': '0.9501', 'policy_loss': '0.9508', 'value_loss': '0.7321', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117468.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '302.3067', 'perf/train_time_sec': '14.6313', 'perf/collect_steps_per_sec': '1.6936', 'perf/collect_game_steps_per_sec': '216.7865', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 70: {'l2_reg': '0.0000', 'loss': '1.6931', 'policy_entropy': '0.9457', 'policy_loss': '0.9476', 'value_loss': '0.7455', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118613.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '316.7954', 'perf/train_time_sec': '14.6367', 'perf/collect_steps_per_sec': '1.6162', 'perf/collect_game_steps_per_sec': '206.8717', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 70: {'pip_count_baseline_avg_outcome': '-0.2500'}\n",
      "Epoch 71: {'l2_reg': '0.0000', 'loss': '1.7158', 'policy_entropy': '0.9657', 'policy_loss': '0.9711', 'value_loss': '0.7448', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118704.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.3445', 'perf/train_time_sec': '14.7563', 'perf/collect_steps_per_sec': '1.6392', 'perf/collect_game_steps_per_sec': '209.8196', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 72: {'l2_reg': '0.0000', 'loss': '1.6956', 'policy_entropy': '0.9622', 'policy_loss': '0.9645', 'value_loss': '0.7311', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118823.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '323.8861', 'perf/train_time_sec': '14.6722', 'perf/collect_steps_per_sec': '1.5808', 'perf/collect_game_steps_per_sec': '202.3427', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 73: {'l2_reg': '0.0000', 'loss': '1.7364', 'policy_entropy': '0.9713', 'policy_loss': '0.9760', 'value_loss': '0.7604', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119075.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.3171', 'perf/train_time_sec': '14.4816', 'perf/collect_steps_per_sec': '1.6289', 'perf/collect_game_steps_per_sec': '208.5028', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 74: {'l2_reg': '0.0000', 'loss': '1.7910', 'policy_entropy': '0.9709', 'policy_loss': '0.9737', 'value_loss': '0.8174', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119327.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.9954', 'perf/train_time_sec': '14.5248', 'perf/collect_steps_per_sec': '1.6358', 'perf/collect_game_steps_per_sec': '209.3833', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: {'l2_reg': '0.0000', 'loss': '1.7975', 'policy_entropy': '0.9680', 'policy_loss': '0.9709', 'value_loss': '0.8266', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119041.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '315.0400', 'perf/train_time_sec': '14.7482', 'perf/collect_steps_per_sec': '1.6252', 'perf/collect_game_steps_per_sec': '208.0244', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 75: {'pip_count_baseline_avg_outcome': '0.7000'}\n",
      "Epoch 76: {'l2_reg': '0.0000', 'loss': '1.7200', 'policy_entropy': '0.9585', 'policy_loss': '0.9624', 'value_loss': '0.7576', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117907.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.8790', 'perf/train_time_sec': '14.6841', 'perf/collect_steps_per_sec': '1.6312', 'perf/collect_game_steps_per_sec': '208.7939', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 77: {'l2_reg': '0.0000', 'loss': '1.6812', 'policy_entropy': '0.9521', 'policy_loss': '0.9544', 'value_loss': '0.7268', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118121.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '315.9312', 'perf/train_time_sec': '14.3827', 'perf/collect_steps_per_sec': '1.6206', 'perf/collect_game_steps_per_sec': '207.4375', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 78: {'l2_reg': '0.0000', 'loss': '1.6471', 'policy_entropy': '0.9444', 'policy_loss': '0.9453', 'value_loss': '0.7018', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118212.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.7003', 'perf/train_time_sec': '14.5532', 'perf/collect_steps_per_sec': '1.6321', 'perf/collect_game_steps_per_sec': '208.9128', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 79: {'l2_reg': '0.0000', 'loss': '1.6440', 'policy_entropy': '0.9499', 'policy_loss': '0.9510', 'value_loss': '0.6930', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117837.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '321.5231', 'perf/train_time_sec': '14.4873', 'perf/collect_steps_per_sec': '1.5924', 'perf/collect_game_steps_per_sec': '203.8298', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 80: {'l2_reg': '0.0000', 'loss': '1.6708', 'policy_entropy': '0.9533', 'policy_loss': '0.9533', 'value_loss': '0.7175', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117817.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '321.4107', 'perf/train_time_sec': '14.4743', 'perf/collect_steps_per_sec': '1.5930', 'perf/collect_game_steps_per_sec': '203.9011', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 80: {'pip_count_baseline_avg_outcome': '-0.1500'}\n",
      "Epoch 81: {'l2_reg': '0.0000', 'loss': '1.6654', 'policy_entropy': '0.9465', 'policy_loss': '0.9478', 'value_loss': '0.7175', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119191.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '316.6714', 'perf/train_time_sec': '14.4134', 'perf/collect_steps_per_sec': '1.6168', 'perf/collect_game_steps_per_sec': '206.9527', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 82: {'l2_reg': '0.0000', 'loss': '1.6577', 'policy_entropy': '0.9396', 'policy_loss': '0.9416', 'value_loss': '0.7160', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118222.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.6228', 'perf/train_time_sec': '14.7692', 'perf/collect_steps_per_sec': '1.6325', 'perf/collect_game_steps_per_sec': '208.9644', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 83: {'l2_reg': '0.0000', 'loss': '1.6705', 'policy_entropy': '0.9483', 'policy_loss': '0.9477', 'value_loss': '0.7228', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117923.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.0147', 'perf/train_time_sec': '14.4901', 'perf/collect_steps_per_sec': '1.6462', 'perf/collect_game_steps_per_sec': '210.7168', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 84: {'l2_reg': '0.0000', 'loss': '1.6819', 'policy_entropy': '0.9351', 'policy_loss': '0.9383', 'value_loss': '0.7437', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118764.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '318.6068', 'perf/train_time_sec': '14.4692', 'perf/collect_steps_per_sec': '1.6070', 'perf/collect_game_steps_per_sec': '205.6955', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 85: {'l2_reg': '0.0000', 'loss': '1.7086', 'policy_entropy': '0.9434', 'policy_loss': '0.9463', 'value_loss': '0.7623', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118569.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '320.5740', 'perf/train_time_sec': '14.3912', 'perf/collect_steps_per_sec': '1.5971', 'perf/collect_game_steps_per_sec': '204.4333', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 85: {'pip_count_baseline_avg_outcome': '0.2500'}\n",
      "Epoch 86: {'l2_reg': '0.0000', 'loss': '1.6984', 'policy_entropy': '0.9488', 'policy_loss': '0.9518', 'value_loss': '0.7466', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117643.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '314.8751', 'perf/train_time_sec': '14.6842', 'perf/collect_steps_per_sec': '1.6260', 'perf/collect_game_steps_per_sec': '208.1333', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 87: {'l2_reg': '0.0000', 'loss': '1.7085', 'policy_entropy': '0.9578', 'policy_loss': '0.9608', 'value_loss': '0.7477', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118419.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '322.9775', 'perf/train_time_sec': '14.5079', 'perf/collect_steps_per_sec': '1.5852', 'perf/collect_game_steps_per_sec': '202.9119', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 88: {'l2_reg': '0.0000', 'loss': '1.6826', 'policy_entropy': '0.9579', 'policy_loss': '0.9612', 'value_loss': '0.7214', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118720.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '323.3546', 'perf/train_time_sec': '14.6276', 'perf/collect_steps_per_sec': '1.5834', 'perf/collect_game_steps_per_sec': '202.6753', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 89: {'l2_reg': '0.0000', 'loss': '1.6940', 'policy_entropy': '0.9517', 'policy_loss': '0.9511', 'value_loss': '0.7429', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117891.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '322.3653', 'perf/train_time_sec': '14.5878', 'perf/collect_steps_per_sec': '1.5883', 'perf/collect_game_steps_per_sec': '203.2973', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: {'l2_reg': '0.0000', 'loss': '1.7531', 'policy_entropy': '0.9552', 'policy_loss': '0.9558', 'value_loss': '0.7973', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117804.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '316.4263', 'perf/train_time_sec': '14.5600', 'perf/collect_steps_per_sec': '1.6181', 'perf/collect_game_steps_per_sec': '207.1130', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 90: {'pip_count_baseline_avg_outcome': '-0.0500'}\n",
      "Epoch 91: {'l2_reg': '0.0000', 'loss': '1.7657', 'policy_entropy': '0.9474', 'policy_loss': '0.9510', 'value_loss': '0.8147', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118645.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.7017', 'perf/train_time_sec': '14.6374', 'perf/collect_steps_per_sec': '1.6426', 'perf/collect_game_steps_per_sec': '210.2523', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 92: {'l2_reg': '0.0000', 'loss': '1.7299', 'policy_entropy': '0.9525', 'policy_loss': '0.9558', 'value_loss': '0.7740', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118100.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.0124', 'perf/train_time_sec': '14.5916', 'perf/collect_steps_per_sec': '1.6357', 'perf/collect_game_steps_per_sec': '209.3719', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 93: {'l2_reg': '0.0000', 'loss': '1.6682', 'policy_entropy': '0.9458', 'policy_loss': '0.9502', 'value_loss': '0.7180', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117917.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '316.5130', 'perf/train_time_sec': '14.7872', 'perf/collect_steps_per_sec': '1.6176', 'perf/collect_game_steps_per_sec': '207.0563', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 94: {'l2_reg': '0.0000', 'loss': '1.6604', 'policy_entropy': '0.9514', 'policy_loss': '0.9550', 'value_loss': '0.7054', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119029.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.5092', 'perf/train_time_sec': '14.7690', 'perf/collect_steps_per_sec': '1.6279', 'perf/collect_game_steps_per_sec': '208.3754', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 95: {'l2_reg': '0.0000', 'loss': '1.6733', 'policy_entropy': '0.9504', 'policy_loss': '0.9519', 'value_loss': '0.7214', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119306.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.5300', 'perf/train_time_sec': '14.6937', 'perf/collect_steps_per_sec': '1.6278', 'perf/collect_game_steps_per_sec': '208.3617', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 95: {'pip_count_baseline_avg_outcome': '0.2000'}\n",
      "Epoch 96: {'l2_reg': '0.0000', 'loss': '1.6869', 'policy_entropy': '0.9480', 'policy_loss': '0.9489', 'value_loss': '0.7380', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117682.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.6383', 'perf/train_time_sec': '14.7543', 'perf/collect_steps_per_sec': '1.6377', 'perf/collect_game_steps_per_sec': '209.6224', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 97: {'l2_reg': '0.0000', 'loss': '1.6807', 'policy_entropy': '0.9408', 'policy_loss': '0.9450', 'value_loss': '0.7357', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117260.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '312.5955', 'perf/train_time_sec': '14.5951', 'perf/collect_steps_per_sec': '1.6379', 'perf/collect_game_steps_per_sec': '209.6511', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 98: {'l2_reg': '0.0000', 'loss': '1.6400', 'policy_entropy': '0.9524', 'policy_loss': '0.9551', 'value_loss': '0.6848', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118273.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.7501', 'perf/train_time_sec': '14.7633', 'perf/collect_steps_per_sec': '1.6371', 'perf/collect_game_steps_per_sec': '209.5475', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 99: {'l2_reg': '0.0000', 'loss': '1.5913', 'policy_entropy': '0.9361', 'policy_loss': '0.9353', 'value_loss': '0.6559', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118969.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '315.5635', 'perf/train_time_sec': '14.7998', 'perf/collect_steps_per_sec': '1.6225', 'perf/collect_game_steps_per_sec': '207.6793', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 100: {'l2_reg': '0.0000', 'loss': '1.6003', 'policy_entropy': '0.9437', 'policy_loss': '0.9431', 'value_loss': '0.6572', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119629.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.1958', 'perf/train_time_sec': '14.6394', 'perf/collect_steps_per_sec': '1.6296', 'perf/collect_game_steps_per_sec': '208.5833', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 100: {'pip_count_baseline_avg_outcome': '-0.9500'}\n",
      "Epoch 101: {'l2_reg': '0.0000', 'loss': '1.6033', 'policy_entropy': '0.9314', 'policy_loss': '0.9333', 'value_loss': '0.6700', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117907.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '316.4182', 'perf/train_time_sec': '14.6985', 'perf/collect_steps_per_sec': '1.6181', 'perf/collect_game_steps_per_sec': '207.1183', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 102: {'l2_reg': '0.0000', 'loss': '1.5726', 'policy_entropy': '0.9351', 'policy_loss': '0.9351', 'value_loss': '0.6375', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118295.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.0650', 'perf/train_time_sec': '14.7352', 'perf/collect_steps_per_sec': '1.6354', 'perf/collect_game_steps_per_sec': '209.3367', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 103: {'l2_reg': '0.0000', 'loss': '1.5974', 'policy_entropy': '0.9340', 'policy_loss': '0.9317', 'value_loss': '0.6657', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118412.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '316.9462', 'perf/train_time_sec': '14.7254', 'perf/collect_steps_per_sec': '1.6154', 'perf/collect_game_steps_per_sec': '206.7733', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 104: {'l2_reg': '0.0000', 'loss': '1.6485', 'policy_entropy': '0.9526', 'policy_loss': '0.9490', 'value_loss': '0.6995', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118313.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.7237', 'perf/train_time_sec': '14.7173', 'perf/collect_steps_per_sec': '1.6268', 'perf/collect_game_steps_per_sec': '208.2334', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105: {'l2_reg': '0.0000', 'loss': '1.6651', 'policy_entropy': '0.9454', 'policy_loss': '0.9445', 'value_loss': '0.7206', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117637.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '319.6617', 'perf/train_time_sec': '14.8104', 'perf/collect_steps_per_sec': '1.6017', 'perf/collect_game_steps_per_sec': '205.0168', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 105: {'pip_count_baseline_avg_outcome': '0.7500'}\n",
      "Epoch 106: {'l2_reg': '0.0000', 'loss': '1.6713', 'policy_entropy': '0.9317', 'policy_loss': '0.9293', 'value_loss': '0.7420', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118603.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.9574', 'perf/train_time_sec': '14.8559', 'perf/collect_steps_per_sec': '1.6256', 'perf/collect_game_steps_per_sec': '208.0789', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 107: {'l2_reg': '0.0000', 'loss': '1.6651', 'policy_entropy': '0.9331', 'policy_loss': '0.9340', 'value_loss': '0.7311', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117457.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '319.1086', 'perf/train_time_sec': '14.6999', 'perf/collect_steps_per_sec': '1.6045', 'perf/collect_game_steps_per_sec': '205.3721', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 108: {'l2_reg': '0.0000', 'loss': '1.6804', 'policy_entropy': '0.9368', 'policy_loss': '0.9372', 'value_loss': '0.7431', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117799.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '314.5603', 'perf/train_time_sec': '14.7821', 'perf/collect_steps_per_sec': '1.6277', 'perf/collect_game_steps_per_sec': '208.3416', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 109: {'l2_reg': '0.0000', 'loss': '1.6447', 'policy_entropy': '0.9400', 'policy_loss': '0.9402', 'value_loss': '0.7045', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118339.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '319.6312', 'perf/train_time_sec': '14.6647', 'perf/collect_steps_per_sec': '1.6018', 'perf/collect_game_steps_per_sec': '205.0363', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 110: {'l2_reg': '0.0000', 'loss': '1.6400', 'policy_entropy': '0.9439', 'policy_loss': '0.9438', 'value_loss': '0.6962', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118191.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '315.3487', 'perf/train_time_sec': '14.7390', 'perf/collect_steps_per_sec': '1.6236', 'perf/collect_game_steps_per_sec': '207.8207', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 110: {'pip_count_baseline_avg_outcome': '0.4500'}\n",
      "Epoch 111: {'l2_reg': '0.0000', 'loss': '1.6379', 'policy_entropy': '0.9374', 'policy_loss': '0.9358', 'value_loss': '0.7021', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118029.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.1600', 'perf/train_time_sec': '14.6736', 'perf/collect_steps_per_sec': '1.6349', 'perf/collect_game_steps_per_sec': '209.2732', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 112: {'l2_reg': '0.0000', 'loss': '1.6556', 'policy_entropy': '0.9411', 'policy_loss': '0.9414', 'value_loss': '0.7142', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118046.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.7188', 'perf/train_time_sec': '15.0674', 'perf/collect_steps_per_sec': '1.6373', 'perf/collect_game_steps_per_sec': '209.5685', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 113: {'l2_reg': '0.0000', 'loss': '1.6485', 'policy_entropy': '0.9539', 'policy_loss': '0.9515', 'value_loss': '0.6970', 'buffer/size': '128000.0000', 'buffer/valid_samples': '116968.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '315.7412', 'perf/train_time_sec': '14.8148', 'perf/collect_steps_per_sec': '1.6216', 'perf/collect_game_steps_per_sec': '207.5624', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 114: {'l2_reg': '0.0000', 'loss': '1.6644', 'policy_entropy': '0.9432', 'policy_loss': '0.9439', 'value_loss': '0.7205', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117760.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '313.5149', 'perf/train_time_sec': '14.7699', 'perf/collect_steps_per_sec': '1.6331', 'perf/collect_game_steps_per_sec': '209.0363', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 115: {'l2_reg': '0.0000', 'loss': '1.6993', 'policy_entropy': '0.9473', 'policy_loss': '0.9448', 'value_loss': '0.7545', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117297.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '318.4628', 'perf/train_time_sec': '14.7398', 'perf/collect_steps_per_sec': '1.6077', 'perf/collect_game_steps_per_sec': '205.7885', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 115: {'pip_count_baseline_avg_outcome': '-0.3500'}\n",
      "Epoch 116: {'l2_reg': '0.0000', 'loss': '1.7082', 'policy_entropy': '0.9439', 'policy_loss': '0.9417', 'value_loss': '0.7665', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118093.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '314.9291', 'perf/train_time_sec': '14.6963', 'perf/collect_steps_per_sec': '1.6258', 'perf/collect_game_steps_per_sec': '208.0976', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 117: {'l2_reg': '0.0000', 'loss': '1.7066', 'policy_entropy': '0.9416', 'policy_loss': '0.9407', 'value_loss': '0.7659', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117915.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '317.5252', 'perf/train_time_sec': '14.4245', 'perf/collect_steps_per_sec': '1.6125', 'perf/collect_game_steps_per_sec': '206.3962', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 118: {'l2_reg': '0.0000', 'loss': '1.6863', 'policy_entropy': '0.9414', 'policy_loss': '0.9404', 'value_loss': '0.7459', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117791.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.9978', 'perf/train_time_sec': '14.5022', 'perf/collect_steps_per_sec': '1.6358', 'perf/collect_game_steps_per_sec': '209.3817', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 119: {'l2_reg': '0.0000', 'loss': '1.6406', 'policy_entropy': '0.9312', 'policy_loss': '0.9293', 'value_loss': '0.7113', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118452.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.7395', 'perf/train_time_sec': '14.4247', 'perf/collect_steps_per_sec': '1.6319', 'perf/collect_game_steps_per_sec': '208.8867', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120: {'l2_reg': '0.0000', 'loss': '1.6774', 'policy_entropy': '0.9352', 'policy_loss': '0.9316', 'value_loss': '0.7457', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118049.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.7599', 'perf/train_time_sec': '14.6518', 'perf/collect_steps_per_sec': '1.6423', 'perf/collect_game_steps_per_sec': '210.2131', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 120: {'pip_count_baseline_avg_outcome': '0.0500'}\n",
      "Epoch 121: {'l2_reg': '0.0000', 'loss': '1.6731', 'policy_entropy': '0.9316', 'policy_loss': '0.9274', 'value_loss': '0.7457', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118535.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '317.7277', 'perf/train_time_sec': '14.6373', 'perf/collect_steps_per_sec': '1.6114', 'perf/collect_game_steps_per_sec': '206.2647', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 122: {'l2_reg': '0.0000', 'loss': '1.6907', 'policy_entropy': '0.9521', 'policy_loss': '0.9495', 'value_loss': '0.7412', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117219.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '315.1570', 'perf/train_time_sec': '14.6600', 'perf/collect_steps_per_sec': '1.6246', 'perf/collect_game_steps_per_sec': '207.9472', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 123: {'l2_reg': '0.0000', 'loss': '1.6662', 'policy_entropy': '0.9417', 'policy_loss': '0.9369', 'value_loss': '0.7293', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117959.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '315.5255', 'perf/train_time_sec': '14.6779', 'perf/collect_steps_per_sec': '1.6227', 'perf/collect_game_steps_per_sec': '207.7043', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 124: {'l2_reg': '0.0000', 'loss': '1.6343', 'policy_entropy': '0.9413', 'policy_loss': '0.9347', 'value_loss': '0.6995', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117693.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '312.7568', 'perf/train_time_sec': '14.7573', 'perf/collect_steps_per_sec': '1.6371', 'perf/collect_game_steps_per_sec': '209.5430', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 125: {'l2_reg': '0.0000', 'loss': '1.6489', 'policy_entropy': '0.9563', 'policy_loss': '0.9532', 'value_loss': '0.6957', 'buffer/size': '128000.0000', 'buffer/valid_samples': '118020.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '311.8676', 'perf/train_time_sec': '14.7215', 'perf/collect_steps_per_sec': '1.6417', 'perf/collect_game_steps_per_sec': '210.1404', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 125: {'pip_count_baseline_avg_outcome': '0.2000'}\n",
      "Epoch 126: {'l2_reg': '0.0000', 'loss': '1.6637', 'policy_entropy': '0.9495', 'policy_loss': '0.9460', 'value_loss': '0.7177', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117816.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '313.9582', 'perf/train_time_sec': '14.6028', 'perf/collect_steps_per_sec': '1.6308', 'perf/collect_game_steps_per_sec': '208.7412', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 127: {'l2_reg': '0.0000', 'loss': '1.6578', 'policy_entropy': '0.9499', 'policy_loss': '0.9481', 'value_loss': '0.7097', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119021.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '311.8047', 'perf/train_time_sec': '14.9897', 'perf/collect_steps_per_sec': '1.6421', 'perf/collect_game_steps_per_sec': '210.1828', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 128: {'l2_reg': '0.0000', 'loss': '1.6710', 'policy_entropy': '0.9446', 'policy_loss': '0.9416', 'value_loss': '0.7294', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117111.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '317.5902', 'perf/train_time_sec': '14.6497', 'perf/collect_steps_per_sec': '1.6121', 'perf/collect_game_steps_per_sec': '206.3540', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 129: {'l2_reg': '0.0000', 'loss': '1.6931', 'policy_entropy': '0.9458', 'policy_loss': '0.9463', 'value_loss': '0.7468', 'buffer/size': '128000.0000', 'buffer/valid_samples': '117592.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '4.0000', 'perf/samples_per_sec': '200.0000', 'perf/collect_time_sec': '307.0701', 'perf/train_time_sec': '14.6413', 'perf/collect_steps_per_sec': '1.6674', 'perf/collect_game_steps_per_sec': '213.4235', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 130: {'l2_reg': '0.0000', 'loss': '1.6875', 'policy_entropy': '0.9525', 'policy_loss': '0.9494', 'value_loss': '0.7381', 'buffer/size': '128000.0000', 'buffer/valid_samples': '119720.0000', 'buffer/fullness_pct': '100.0000', 'perf/train_steps_per_sec': '512000000.0000', 'perf/samples_per_sec': '25600000000.0000', 'perf/collect_time_sec': '318.7504', 'perf/train_time_sec': '14.5954', 'perf/collect_steps_per_sec': '1.6063', 'perf/collect_game_steps_per_sec': '205.6028', 'game/terminated_count': '0.0000', 'game/terminated_pct': '0.0000'}\n",
      "Epoch 130: {'pip_count_baseline_avg_outcome': '-0.2000'}\n"
     ]
    }
   ],
   "source": [
    "output = trainer.train_loop(seed=0, num_epochs=150, eval_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and GIFs generated will appear in the same directory as this notebook, and also on your `wandb` dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbozero",
   "language": "python",
   "name": "turbozero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
