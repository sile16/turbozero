{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World, TurboZero ðŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`turbozero` provides a vectorized implementation of AlphaZero. \n",
    "\n",
    "In a nutshell, this means we can massively speed up training, by collecting many self-play games and running Monte Carlo Tree Search in parallel across one or more GPUs!\n",
    "\n",
    "As the user, you just need to provide:\n",
    "* environment dynamics functions (step and init) that adhere to the TurboZero spec\n",
    "* a conversion function for environment state -> neural net input\n",
    "* and a few hyperparameters!\n",
    "\n",
    "TurboZero takes care of the rest. ðŸ˜€ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Follow the instructions in the repo readme to properly install dependencies and set up your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "In order to take advantage of the batched implementation of AlphaZero, we need to pair it with a vectorized environment.\n",
    "\n",
    "Fortunately, there are many great vectorized RL environment libraries, one I like in particular is [pgx](https://github.com/sotetsuk/pgx).\n",
    "\n",
    "Let's use the 'othello' environment. You can see its documentation here: https://sotets.uk/pgx/othello/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 3.12.10 (main, May 21 2025, 10:26:13) [GCC 13.3.0]\n",
      "Jax Version: 0.6.1, Backend: gpu \n",
      "PGX Version 2.5.10\n"
     ]
    }
   ],
   "source": [
    "import pgx\n",
    "\n",
    "import sys\n",
    "print(f\"Python version {sys.version}\")\n",
    "\n",
    "#jax.config.update('jax_platform_name', 'gpu')\n",
    "import jax\n",
    "\n",
    "env = pgx.make('othello')\n",
    "\n",
    "#print(f\"Env: {os.environ}\")\n",
    "print(f\"Jax Version: {jax.__version__}, Backend: {jax.default_backend()} \")\n",
    "print(f\"PGX Version {pgx.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Dynamics\n",
    "\n",
    "Turbozero needs to interface with the environment in order to build search trees and collect self-play episodes.\n",
    "\n",
    "We can define this interface with the following functions:\n",
    "* `env_step_fn`: given an environment state and an action, return the new environment state \n",
    "```python\n",
    "    EnvStepFn = Callable[[chex.ArrayTree, int], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "* `env_init_fn`: given a key, initialize and reutrn a new environment state\n",
    "```python\n",
    "    EnvInitFn = Callable[[chex.PRNGKey], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "Fortunately, environment libraries implement these for us! We just need to extract a few key pieces of information \n",
    "from the environment state so that we can match the TurboZero specification. We store this in a StepMetadata object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@chex.dataclass(frozen=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
       "\u001b[38;5;28;01mclass\u001b[39;00m StepMetadata:\n",
       "    \u001b[33m\"\"\"Metadata for a step in the environment.\u001b[39m\n",
       "\u001b[33m    - `rewards`: rewards received by the players\u001b[39m\n",
       "\u001b[33m    - `action_mask`: mask of valid actions\u001b[39m\n",
       "\u001b[33m    - `terminated`: whether the environment is terminated\u001b[39m\n",
       "\u001b[33m    - `cur_player_id`: current player id\u001b[39m\n",
       "\u001b[33m    - `step`: step number\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    rewards: chex.Array\n",
       "    action_mask: chex.Array\n",
       "    terminated: bool\n",
       "    cur_player_id: int\n",
       "    step: int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from core.types import StepMetadata\n",
    "%psource StepMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `rewards` stores the rewards emitted for each player for the given timestep\n",
    "* `action_mask` is a mask across all possible actions, where legal actions are set to `True`, and invalid/illegal actions are set to `False`\n",
    "* `terminated` True if the environment is terminated/completed\n",
    "* `cur_player_id`: id of the current player\n",
    "* `step`: step number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the environment interface for `Othello` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_fn(state, action, key):\n",
    "    new_state = env.step(state, action, key)\n",
    "    return new_state, StepMetadata(\n",
    "        rewards=new_state.rewards,\n",
    "        action_mask=new_state.legal_action_mask,\n",
    "        terminated=new_state.terminated,\n",
    "        cur_player_id=new_state.current_player,\n",
    "        step = new_state._step_count\n",
    "    )\n",
    "\n",
    "def init_fn(key):\n",
    "    state = env.init(key)\n",
    "    return state, StepMetadata(\n",
    "        rewards=state.rewards,\n",
    "        action_mask=state.legal_action_mask,\n",
    "        terminated=state.terminated,\n",
    "        cur_player_id=state.current_player,\n",
    "        step = state._step_count\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Next, we'll need to define the architecture of the neural network \n",
    "\n",
    "A simple implementation of the residual neural network used in the _AlphaZero_ paper is included for your convenience. \n",
    "\n",
    "You can implement your own architecture using `flax.linen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.networks.azresnet import AZResnetConfig, AZResnet\n",
    "\n",
    "resnet = AZResnet(AZResnetConfig(\n",
    "    policy_head_out_size=env.num_actions,\n",
    "    num_blocks=4,\n",
    "    num_channels=32,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to convert our environment's state into something our neural network can take as input (i.e. structured data -> Array). `pgx` conveniently includes this in `state.observation`, but for other environments you may need to perform the conversion yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_nn_input(state):\n",
    "    return state.observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Next, we can initialize our evaluator, AlphaZero, which takes the following parameters:\n",
    "\n",
    "* `eval_fn`: function used to evaluate a leaf node (returns a policy and value)\n",
    "* `num_iterations`: number of MCTS iterations to run before returning the final policy\n",
    "* `max_nodes`: maximum capacity of search tree\n",
    "* `branching_factor`: branching factor of search tree == policy_size\n",
    "* `action_selector`: the algorithm used to select an action to take at any given search node, choose between:\n",
    "    * `PUCTSelector`: AlphaZero action selection algorithm\n",
    "    * `MuZeroPUCTSelector`: MuZero action selection algorithm\n",
    "    * or write your own! :)\n",
    "\n",
    "There are also a few other optional parameters, a few of the important ones are:\n",
    "* `temperature`: temperature applied to move probabilities prior to sampling (0.0 == argmax, ->inf == completely random sampling). I reccommend setting this to 1.0 for training (default) and 0.0 for evaluation.\n",
    "* `dirichlet_alpha`: magnitude of Dirichlet noise to add to root policy (default 0.3). Generally, the more actions are possible in a game, the smaller this value should be. \n",
    "* `dirichlet_epsilon`: proportion of root policy composed of Dirichlet noise (default 0.25)\n",
    "\n",
    "\n",
    "We use `make_nn_eval_fn` to create a leaf evaluation function that uses our neural network to generate a policy and a value for the given state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from core.evaluators.alphazero import AlphaZero\n",
    "from core.evaluators.evaluation_fns import make_nn_eval_fn\n",
    "from core.evaluators.mcts.action_selection import PUCTSelector\n",
    "from core.evaluators.mcts.mcts import MCTS\n",
    "from core.evaluators.mcts.stochastic_mcts import StochasticMCTS\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "# alphazero can take an arbirary search `backend`\n",
    "# here we use classic MCTS\n",
    "az_evaluator = StochasticMCTS(\n",
    "    eval_fn = make_nn_eval_fn(resnet, state_to_nn_input),\n",
    "    num_iterations = 32,\n",
    "    max_nodes = 40,\n",
    "    branching_factor = env.num_actions,\n",
    "    action_selector = PUCTSelector(),\n",
    "    temperature = 1.0,\n",
    "    stochastic_action_probs = jnp.array([1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a separate evaluator with different parameters to use for testing purposes. We'll give this one a larger budget (num_iterations), and set the temperature to zero so it always chooses the most-visited action after search is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.evaluators.alphazero import AlphaZero\n",
    "\n",
    "az_evaluator_test = AlphaZero(MCTS)(\n",
    "    eval_fn = make_nn_eval_fn(resnet, state_to_nn_input),\n",
    "    num_iterations = 64,\n",
    "    max_nodes = 80,\n",
    "    branching_factor = env.num_actions,\n",
    "    action_selector = PUCTSelector(),\n",
    "    temperature = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "We can also test our trained model periodically against baselines, in order to gauge improvement.\n",
    "\n",
    "Conveniently, pgx offers pre-trained baseline models for certain environments. If we want to test against one, we can use `make_nn_eval_fn_no_params_callable`, which just returns an evaluation function that uses the baseline model to evaluate the game state.\n",
    "\n",
    "We can combine this with the `AlphaZero` evaluator like we did before to create a competeing AlphaZero instance to play against. Other than the eval_fn, it is important to use the same parameters that we give to our (test) evaluator, so as to give a true comparison between the strength of the policy/value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.evaluators.evaluation_fns import make_nn_eval_fn_no_params_callable\n",
    "\n",
    "model = pgx.make_baseline_model('othello_v0')\n",
    "\n",
    "baseline_eval_fn = make_nn_eval_fn_no_params_callable(model, state_to_nn_input)\n",
    "\n",
    "baseline_az = AlphaZero(MCTS)(\n",
    "    eval_fn = baseline_eval_fn,\n",
    "    num_iterations = 64,\n",
    "    max_nodes = 80,\n",
    "    branching_factor = env.num_actions,\n",
    "    action_selector = PUCTSelector(),\n",
    "    temperature = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see baselines available for other pgx environments, see https://sotets.uk/pgx/api/#pgx.BaselineModelId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use similar ideas to write a greedy baseline evaluation function, one that doesn't use a neural network at all!\n",
    "\n",
    "Instead, it simply counts the number of tiles for the active player and compares it to the number of tiles controlled by the other player, so the value is higher for states where the active player controls more tiles than the other player.\n",
    "\n",
    "Using similar techniques as before, we can create another AlphaZero evaluator to test against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def greedy_eval(obs):\n",
    "    value = (obs[...,0].sum() - obs[...,1].sum()) / 64\n",
    "    return jnp.ones((1,env.num_actions)), jnp.array([value])\n",
    "\n",
    "greedy_baseline_eval_fn = make_nn_eval_fn_no_params_callable(greedy_eval, state_to_nn_input)\n",
    "\n",
    "\n",
    "greedy_az = AlphaZero(MCTS)(\n",
    "    eval_fn = greedy_baseline_eval_fn,\n",
    "    num_iterations = 64,\n",
    "    max_nodes = 80,\n",
    "    branching_factor = env.num_actions,\n",
    "    action_selector = PUCTSelector(),\n",
    "    temperature = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory Buffer\n",
    "\n",
    "Next, we'll initialize a replay memory buffer to hold selfplay trajectories that we can sample from during training. This actually just defines an interface, the buffer state itself will be initialized and managed internally.\n",
    "\n",
    "The replay buffer is batched, it retains a buffer of trajectories across a batch dimension. We specify a `capacity`: the amount of samples stored in a single buffer. The total capacity of the entire replay buffer is then `batch_size * capacity`, where `batch_size` is the number of environments/self-play games being run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.memory.replay_memory import EpisodeReplayBuffer\n",
    "\n",
    "replay_memory = EpisodeReplayBuffer(capacity=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation (Optional)\n",
    "\n",
    "During self-play, we allow for any number of custom data augmentation functions, in order to create more training samples. \n",
    "\n",
    "In RL, it's sometimes common to take advantage of rotations or symmetries in order to generate additional training examples. \n",
    "\n",
    "In Othello, we could simply consider rotating the board to generate a new training example, we will need to be careful to update our policy as well.\n",
    "\n",
    "In order to implement a data augmentation function, we must follow `DataTransformFn`:\n",
    "```python\n",
    "# (policy mask, policy weights, environment state) -> (transformed policy mask, transformed policy weights, transformed environment state)\n",
    "DataTransformFn = Callable[[chex.Array, chex.Array, chex.ArrayTree], Tuple[chex.Array, chex.Array, chex.ArrayTree]]\n",
    "```\n",
    "\n",
    "We create rotational transform functions for rotating 90, 180, 270 degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rot_transform_fn(amnt: int):\n",
    "    def rot_transform_fn(mask, policy, state):\n",
    "        action_ids = jnp.arange(65) # 65 total actions, but only rotate the first 64! (65th is always do nothing action)\n",
    "        # we only use state.observation, no need to update the rest of the state fields\n",
    "        new_obs = jnp.rot90(state.observation, amnt, axes=(-3,-2))\n",
    "        # map action ids to new action ids\n",
    "        idxs = jnp.arange(64).reshape(8,8) # rotate first 64 actions\n",
    "        new_idxs = jnp.rot90(idxs, amnt, axes=(0, 1)).flatten()\n",
    "        action_ids = action_ids.at[:64].set(new_idxs)\n",
    "        # get new mask and policy\n",
    "        new_mask = mask[...,action_ids]\n",
    "        new_policy = policy[...,action_ids]\n",
    "        return new_mask, new_policy, state.replace(observation=new_obs)\n",
    "\n",
    "    return rot_transform_fn\n",
    "\n",
    "# make transform fns for rotating 90, 180, 270 degrees\n",
    "transforms = [make_rot_transform_fn(i) for i in range(1,4)]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering\n",
    "We can optionally provide a `render_fn` that will record games played by our model against one of the baselines and save it as a `.gif`.\n",
    "\n",
    "I've included a helper fn that takes care of this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function depends upon cairoSVG, which itself depends upon `cairo`, which you'll need to install on your system.\n",
    "\n",
    "On Ubuntu, this can be done with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! apt-get update && apt-get -y install libcairo2-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're on another OS, consult https://www.cairographics.org/download/ for installation guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from core.testing.utils import render_pgx_2p\n",
    "render_fn = partial(render_pgx_2p, p1_label='Black', p2_label='White', duration=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Initialization\n",
    "Now that we have all the proper pieces defined, we are ready to initialize a Trainer and start training!\n",
    "\n",
    "The `Trainer` takes many parameters, so let's walk through them all:\n",
    "* `batch_size`: # of parallel environments used to collect self-play games\n",
    "* `train_batch_size`: size of minibatch used during training step\n",
    "* `warmup_steps`: # of steps (per batch) to collect via self-play prior to entering the training loop. This is used to populate the replay memory with some initial samples\n",
    "* `collection_steps_per_epoch`: # of steps (per batch) to collect via self-play per epoch\n",
    "* `train_steps_per_epoch`: # of train steps per epoch\n",
    "* `nn`: neural network (`linen.Module`)\n",
    "* `loss_fn`: loss function used for training, we use a provided default loss which implements the loss function used in the `AlphaZero` paper\n",
    "* `optimizer`: an `optax` optimizer used for training\n",
    "* `evaluator`: the `Evaluator` to use during self-play, we initialized ours using `AlphaZero(MCTS)`\n",
    "* `memory_buffer`: the memory buffer used to store samples from self-play games, we  initialized ours using `EpisodeReplayBuffer`\n",
    "* `max_episode_steps`: maximum number of steps/turns to allow before truncating an episode\n",
    "* `env_step_fn`: environment step function (we defined ours above)\n",
    "* `env_init_fn`: environment init function (we defined ours above)\n",
    "* `state_to_nn_input_fn`: function to convert environment state to nn input (we defined ours above)\n",
    "* `testers`: any number of `Tester`s, used to evaluate a given model and take their own parameters. We'll use the two evaluators defined above to initialize two Testers.\n",
    "* `evaluator_test`: (Optional) Evaluator used within Testers. By default used `evaluator`, but sometimes you may want to test with a larger MCTS iteration budget for example, or a different move sampling temperature\n",
    "* `data_transform_fns`: (optional) list of data transform functions to apply to self-play experiences (e.g. rotation, reflection, etc.)\n",
    "* `extract_model_params_fn`: (Optional) in special cases we need to define how to extract all model parameters from a flax `TrainState`. The default function handles BatchNorm, but if another special-case technique applied across batches is used (e.g. Dropout) we would need to define a function to extract the appropriate parameters. You usually won't need to define this!\n",
    "* `wandb_project_name`: (Optional) Weights and Biases project name. You will be prompted to login if a name is provided. If a name is provided, a run will be initialized and loss and other metrics will be logged to the given wandb project.\n",
    "* `ckpt_dir`: (Optional) directory to store checkpoints in, by default this is set to `/tmp/turbozero_checkpoints`\n",
    "* `max_checkpoints`: (Optional) maximum number of most-recent checkpoints to retain (default: 2)\n",
    "* `num_devices`: (Optional) number of hardware accelerators (GPUs/TPUs) to use. If not given, all available hardware accelerators are used\n",
    "* `wandb_run`: (Optional) continues from an initialized `wandb` run if provided, otherwise a new one is initialized\n",
    "* `extra_wandb_config`: (Optional) any extra metadata to store in the `wandb` run config\n",
    "\n",
    "A training epoch is comprised of M collection steps, followed by N training steps sampling minibatches from replay memory. Optionally, any number of Testers evaluate the current model. At the end of each epoch, a checkpoint is saved.\n",
    "\n",
    "If you are using one or more GPUs (reccommended), TurboZero by default will run on all your available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msile16\u001b[0m (\u001b[33msile16-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile/github/turbozero/notebooks/wandb/run-20250608_105907-e25zrhqg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sile16-self/turbozero-othello/runs/e25zrhqg' target=\"_blank\">lucky-planet-27</a></strong> to <a href='https://wandb.ai/sile16-self/turbozero-othello' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sile16-self/turbozero-othello' target=\"_blank\">https://wandb.ai/sile16-self/turbozero-othello</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sile16-self/turbozero-othello/runs/e25zrhqg' target=\"_blank\">https://wandb.ai/sile16-self/turbozero-othello/runs/e25zrhqg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from core.testing.two_player_baseline import TwoPlayerBaseline\n",
    "from core.training.loss_fns import az_default_loss_fn\n",
    "from core.training.stochastic_train import StochasticTrainer\n",
    "import optax\n",
    "\n",
    "trainer = StochasticTrainer(\n",
    "    batch_size = 1024,\n",
    "    train_batch_size = 4096,\n",
    "    warmup_steps = 0,\n",
    "    collection_steps_per_epoch = 256,\n",
    "    train_steps_per_epoch = 64,\n",
    "    nn = resnet,\n",
    "    loss_fn = partial(az_default_loss_fn, l2_reg_lambda = 0.0),\n",
    "    optimizer = optax.adam(1e-3),\n",
    "    evaluator = az_evaluator,\n",
    "    memory_buffer = replay_memory,\n",
    "    max_episode_steps = 80,\n",
    "    env_step_fn = step_fn,\n",
    "    env_init_fn = init_fn,\n",
    "    state_to_nn_input_fn=state_to_nn_input,\n",
    "    testers = [\n",
    "        TwoPlayerBaseline(num_episodes=128, baseline_evaluator=baseline_az, name='pretrained'), #render_fn=render_fn, render_dir='.'\n",
    "        TwoPlayerBaseline(num_episodes=128, baseline_evaluator=greedy_az, name='greedy'),\n",
    "    ],\n",
    "    evaluator_test = az_evaluator_test,\n",
    "    data_transform_fns=transforms,\n",
    "    wandb_project_name = 'turbozero-othello' \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_temperature(train_steps) -> float:\n",
    "    \n",
    "    if train_steps < 1e6:\n",
    "        return 1.0\n",
    "    elif train_steps < 2e6:\n",
    "        return 0.5\n",
    "    elif train_steps < 3e6:\n",
    "        return 0.1\n",
    "    else:\n",
    "        # Greedy selection.\n",
    "        return 0.0\n",
    "\n",
    "trainer.set_temp_fn(get_temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now all that's left to do is to kick off the training loop! We need to pass an initial seed for reproducibility, and the number of epochs to run for!\n",
    "\n",
    "If you've set up `wandb`, you can track metrics via plots in the run dashboard. Metrics will also be printed to the console. \n",
    "\n",
    "IMPORTANT: The first epoch will not execute quickly! This is because there is significant overhead in JAX compilation (nearly all of the training loop is JIT-compiled). This will cause the first epoch to run very slowly, as JIT-compiled functions are traced and compiled the first time they are run. Expect epochs after the first to execute much more quickly. Typically, GPU utilization will also be low/zero during this period.\n",
    "\n",
    "It's also worth mentioning that the hyperparameters in this notebook are just here for example purposes. Regardless of the task, they will need to be tuned according to the characteristics of the environment as well as your available hardware and time/cost constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 1.0\n",
      "Collecting self-play games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 10:59:16.326141: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-06-08 10:59:17.383052: E external/xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-06-08 10:59:17.467098: E external/xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-06-08 10:59:17.626754: E external/xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-06-08 10:59:17.708433: E external/xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-06-08 10:59:17.817209: E external/xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-06-08 10:59:17.895496: E external/xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-06-08 10:59:18.002211: E external/xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-06-08 10:59:18.079751: E external/xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 10:59:46.412417: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Done\n",
      "Epoch 0: {'l2_reg': '0.0000', 'loss': '2.1261', 'policy_entropy': '1.7103', 'policy_loss': '1.6963', 'value_loss': '0.4297', 'train/train_time_sec': '12.3277', 'train/train_steps_per_sec': '21264.5781', 'collect/temperature': '1.0000', 'collect/collect_time_sec': '31.6439', 'collect/collect_steps_per_sec': '8.0900', 'buffer/populated': '262144.0000', 'buffer/trainable_samples': '247644.0000', 'buffer/fullness_pct': '25.6000', 'buffer/trainable_pct': '24.1840', 'buffer/games_completed': '4099.0000', 'buffer/avg_game_length': '60.4157', 'perf/epoch_time_sec': '44.2200'}\n",
      "Testing\n",
      "Epoch 0: {'pretrained_avg_outcome': '-0.9844', 'perf/test_pretrained_time_sec': '54.3892'}\n",
      "Epoch 0: {'greedy_avg_outcome': '0.3203', 'perf/test_greedy_time_sec': '32.4512'}\n",
      "Temperature: 1.0\n",
      "Collecting self-play games\n"
     ]
    }
   ],
   "source": [
    "output = trainer.train_loop(seed=0, num_epochs=100, eval_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and GIFs generated will appear in the same directory as this notebook, and also on your `wandb` dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbov4",
   "language": "python",
   "name": "turbov4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
